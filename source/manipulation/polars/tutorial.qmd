::: {.content-visible when-profile="fr"}
# Traiter des donn√©es tabulaires avec Polars

L'analyse statistique a g√©n√©ralement pour base des **donn√©es tabulaires**, dans lesquelles chaque ligne repr√©sente une observation et chaque colonne une variable.
Pour traiter ce type de donn√©es et y appliquer facilement les m√©thodes d'analyse de donn√©es standards, des objets d√©di√©s ont √©t√© con√ßus : les `DataFrames`.
Les utilisateurs de `R` connaissent bien cette structure de donn√©es, qui est native √† ce langage orient√© statistique.
En `Python`, langage g√©n√©raliste, cet objet n'existe pas nativement.
Plusieurs packages viennent ainsi combler ce manque, comme `pandas`, XX.

## Les atouts de Polars

Les _benchmarks_ disponibles [sont clairs](https://h2oai.github.io/db-benchmark/) : `Polars` est
un ours qui court vite !

Le benchmark suivant, [effectu√© par H2O](https://h2oai.github.io/db-benchmark/), propose
un comparatif de la vitesse des principaux frameworks de manipulation de donn√©es
pour effectuer une agr√©gation par groupe avec un jeu de donn√©es de 50GB:

![Benchmark H2O](img/polars-benchmark-short.png)

`Polars` devance des solutions connues pour leur efficacit√© sur ce type d'op√©rations,
comme le package `R` `data.table`. L'utilisateur habituel de `Pandas`
ne pourrait m√™me pas traiter ces donn√©es, qui exc√®dent les capacit√©s computationnelles
de sa machine.

### L'√©valuation _lazy_

Plusieurs √©l√©ments expliquent cette rapidit√©.

En premier lieu, `Polars` est con√ßu pour optimiser les requ√™tes :
gr√¢ce au mode _lazy_ (_"paresseux"_),
on laisse la possibilit√© au moteur d'analyser ce qu'on souhaite faire
pour proposer une ex√©cution optimale (pour la lecture comme pour la transformation des jeux de donn√©es).
La _lazy evaluation_ est une m√©thode assez commune pour am√©liorer la vitesse
des traitements et est utilis√©e, entre autres, par `Spark`.

Du fait de
la _lazy evaluation_ il est ainsi possible, par exemple,
si un filtre sur les lignes arrive
tardivement, de le remonter dans l'ordre des op√©rations effectu√©es par `Python`
afin que
les op√©rations ult√©rieures ne soient effectu√©es que sur
l'ensemble optimal de donn√©es.
Ces optimisations sont d√©taill√©es dans la [documentation officielle](https://pola-rs.github.io/polars-book/user-guide/optimizations/intro.html).


### Lecture optimis√©e des fichiers

L'utilisateur `Pandas` est habitu√© √† lire du CSV avec `pd.read_csv`.
Avec `Polars`, il existe deux mani√®res, tr√®s ressemblantes
de le faire.

```python
import polars as pl

# Cr√©ation d'une requ√™te
q = (
    pl.scan_csv("iris.csv") # Lecture lazy
    .filter(pl.col("sepal_length") > 5)
    .groupby("species")
    .agg(pl.all().sum())
)

# Ex√©cution de la requ√™te
df = q.collect()
```

Avec cette syntaxe, les connaisseurs de `Pyspark` retrouveront facilement leurs petits (ours üêª).

On peut toujours lire de mani√®re plus directe
(en mode _eager_, _"impatient"_) en utilisant la fonction `read_csv`,
et ensuite appliquer des transformations optimisables en glissant habilement `lazy` :

```python
df = pl.read_csv("iris.csv")

df_res = df.lazy() # ‚Üê  ici :)
  .filter(pl.col("sepal_length") > 5)
  .groupby("species")
  .agg(pl.all().sum())
  .collect()
```

`Polars` fonctionne √©galement tr√®s bien avec le format `Parquet`, comme illustr√© dans
le _notebook_ qui accompagne ce _post_.


### Parall√©lisation

`Polars` parall√©lise les traitements d√®s que cela est possible, notamment dans le cas d'agr√©gation.
Chaque coeur se charge d'une partie de l'agr√©gation et envoie des donn√©es plus l√©g√®res √† `Python`
qui va finaliser l'agr√©gation.

![Parall√©lisation](img/polars-split-parallel-apply-combine.svg)

_Illustration du principe de la parall√©lisation_

Sur les syst√®mes proposant de nombreux coeurs, cela peut faire gagner beaucoup de temps.

### Des couches basses √† la pointe

Enfin, le choix d'utiliser √† la fois le format de repr√©sentation en m√©moire [Arrow](https://arrow.apache.org/) et le langage [Rust](https://www.rust-lang.org/fr) pour le coeur de la biblioth√®que n'est pas √©tranger √† cette performance.

### Calculs _out of memory_

`Polars` travaille vite mais pr√©sente aussi l'avantage
de lire naturellement des jeux de donn√©es hors des limites de la m√©moire de l'ordinateur gr√¢ce √† [sa capacit√© de lire en flux](https://www.youtube.com/watch?v=3-C0Afs5TXQ) (m√©thode qu'on appelle le _streaming_).


```python
# La m√™me requ√™te que tout √† l'heure va lire le fichier "en flux"
df = q.collect(streaming=True)
```

De plus, `Polars` lit nativement les fichiers `Parquet` qui par ses propri√©t√©s
permet d'aller beaucoup plus vite que le CSV !

# Une API fluide

C'est un reproche r√©guli√®rement fait √† `Pandas` :
la syntaxe de manipulations des donn√©es est parfois complexe ou peu lisible, et les choix d'√©criture ne sont pas transparents du point de vue des performances.

L'API propos√©e par Polars est √† la fois expressive et transparente.
Voici un exemple d'exploitation de la BPE, issu du _notebook_ accompagnant
ce _post_ :

```python
df.lazy()
  .filter(
    pl.col("TYPEQU") == "B316"
  )
  .groupby("DEP")
  .agg(
    pl.count().alias("NB_STATION_SERVICE")
  )
  .collect()
```

# Un pipe natif
On retrouve une s√©mantique d'op√©rations de haut niveau qui s'encha√Ænent √† la mani√®re de [ce que l'on peut faire en `dplyr`](https://www.book.utilitr.org/03_fiches_thematiques/fiche_tidyverse#comment-utiliser-lop%C3%A9rateur-pipe-avec-le-tidyverse).





On commence par importer la librairie `Polars`. L'usage est courant est de lui attribuer l'alias `pl` afin de simplifier les futurs appels aux objets et fonctions du package.

Polars plus proche de l'univers du tidyverse pour des personnes venant de R.
beaucoup plus rapide que pandas
plus jeune (et donc pas (encore?) int√©gr√© partout ? panda)
moins r√©pandu que Pandas
tr√®s strict sur les types des colonnes


Parler de :
- colonnes sont associ√©es √† des string.
- pas d'index.
- polars selectors
- lazy eval
- unnest

"TO DO" - Polars S√©ries

pour aller plus loin :
- interconnection avec duckDB pour des requ√™tes en SQL

Ressources :
- [Blog Awesome Polars](https://ddotta.github.io/awesome-polars/), par Damien D
- [Notebook Polars](https://datalab.sspcloud.fr/launcher/ide/jupyter-python?name=Tutoriel%20Polars&version=2.3.28&s3=region-79669f20&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2Fntoulemonde%2Ftest%2Frefs%2Fheads%2Fmain%2Finit.sh¬ª&autoLaunch=true)


::: {.panel-tabset group="polars_vs_tidypolars"}}

## Polars
```{python}
import polars as pl
import numpy as np
```

## Tidypolars
Pour celles et ceux qui veulent utiliser des fonctions pressqu'identiques √† celles du tidyverse,
il y a le package [tidypolars](https://tidypolars.readthedocs.io/en/latest/).

Les noms des fonctions dans tidypolars sont presque tous identiques √† ceux du tidyverse:
- un DataFrame de Polars devient un tibble chez tidypolars;
- la structure de la cr√©ation d'un tibble est la m√™me que dans R;
- les m√©thodes filter, select de Polars restent les m√™mes;
- la m√©thode with_columns de Polars devient mutate  ...

Tidypolars reprend ainsi exactement la syntaxe des principales fonctions de dplyr, tidyr, stringr, lubridate, tidyselect directement dans Python!
La liste compl√®te des fonctions reprises est disponible dans la [documentation](https://tidypolars.readthedocs.io/en/latest/reference.html).

La seule diff√©rence notable entre tidypolars et R est li√©e aux colonnes, qui sont accessibles par pl.col/tp.col dans polars/tidypolars quand dans R leur nom seul suffit.

Pour la simplicit√© du tutoriel, le tutoriel pr√©sente le d√©but avec tidypolars mais ensuite  utilise uniquement Polars.
Pour plus de d√©tail, allez voir la [documentation](https://tidypolars.readthedocs.io/en/latest/reference.html) !

```{python}
import polars as pl
import tidypolars as tp

from tidypolars import col
```

:::


## Structures de donn√©es

### Le `DataFrame`

Fondamentalement, un DataFrame consiste en une collection de Series. Cette concat√©nation construit donc une table de donn√©es, dont les Series correspondent aux colonnes. La figure suivante ([source](https://www.geeksforgeeks.org/creating-a-pandas-dataframe/)) permet de bien comprendre cette structure de donn√©es.

![](img/structure_df.png){fig-align="center" width="800"}

Un DataFrame peut √™tre construit de multiples mani√®res. En pratique, on construit g√©n√©ralement un DataFrame directement √† partir de fichiers de donn√©es tabulaires (ex : CSV, excel), rarement √† la main. On illustrera donc seulement la m√©thode de construction manuelle la plus usuelle : √† partir d'un dictionnaire de donn√©es.

::: {.panel-tabset group="polars_vs_tidypolars"}

## Polars

```{python}
df = pl.DataFrame(
    data = {
        "var1": [1.3, 5.6, None, None, 0, None],
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }, strict=False
)

df
```

## Tidypolars

```{python}

df = tp.tibble(
    var1 = [1.3, 5.6, None, None, 0, None],
    experiment = ["test", "train", "test", "train", "train", "validation"],
    date = ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
    sample = "sample1"
)

df

```

:::


A noter qu'une valeur manquante est indiqu√©e par `None` en Polars. Elle sera ensuite traduite en type Null interne √† Polars.

Un DataFrame Polars dispose d'un ensemble d'attributs utiles que nous allons d√©couvrir tout au long de ce tutoriel. Pour l'instant, int√©ressons-nous aux plus basiques : le nom des colonnes.

```{python}
df.columns
```

Les principaux autres attributs d'un dataframe sont :
- li√©s √† la taille du data frame :
    - le nombre de ligne (df.height) et de colonnes (df.width)
    - la forme globale (df.shape), qui reprend le nombre de lignes et de colonnes
- li√©s aux formats des colonnes:
    - le sch√©ma du dataframe (df.schema)
    - les formats des colonnes (df.dtypes)

## S√©lectionner des donn√©es

Lors de la manipulation des donn√©es tabulaires, il est fr√©quent de vouloir extraire des colonnes sp√©cifiques d'un `DataFrame`.
Cette extraction suit la m√™me logique que le tidyverse avec `Polars`.
La m√©thode `select` permet de s√©lectionner sur les colonnes quand `filter` filtre les lignes.

### S√©lectionner des colonnes

Que l'on veuille s√©lectionner une ou deux colonnes en `Polars`, la logique est similaire.

Pour extraire une seule colonne, on peut utiliser la syntaxe suivante :

```{python}
selected_column = df.select("var1")
selected_column
```

L'objet `selected_column` renvoie ici la colonne nomm√©e `var1` du `DataFrame` `df`.
Contrairement √† Pandas, ici on ne garde qu'une seule colonne du dataframe, on ne l'extrait pas.
L'objet `selected_columns` reste donc un dataframe de 1 colonne, et n'est pas une Polars Series.
En effet, si on s√©lectionnait 2 colonnes, cela serait aussi dataframe.

::: {.callout-tip}
Si vous voulez transformer un dataframe en une s√©rie Polars, il faut utiliser la m√©thode `to_series()` ou reprendre la syntaxe de Pandas qui marche aussi en Polars :

```{python}
print(f'En passant par select.to_series(), la table \n {df.select('var1').to_series()} \n est de type {type(df.select('var1').to_series())}')
print(f'En passant par df[var1], la table \n {df['var1']} \n est aussi de type {type(df['var1'])}')
```

:::


Pour extraire plusieurs colonnes, il suffit de passer une liste des noms des colonnes souhait√©es
ou sans passer par une liste, comme dans le tidyverse.

Cet extrait montre les colonnes `var1` et `experiment` du `DataFrame` `df`.

```{python}
print(df.select("var1", "experiment"))
print(df.select(["var1", "experiment"]))
```


### S√©lectionner des lignes

En pratique, on souhaite souvent filtrer un DataFrame selon certaines conditions.
Dans ce cas, on se sert principalement de filtres bool√©ens selon une logique similaire au tidyverse.
La principale diff√©rence tient au nommage des colonnes.
En effet, le nom de chaque colonne doit √™tre entour√©e d'un s√©lecteur `pl.col`.

Quand en R on √©crirait

```r
df |>
    filter(var1>=0)
```

en Polars, on va l'√©crire

```{python}
print(df.filter(pl.col('var1')>=0))
print(df.filter(pl.col.var1 >= 0))
```

Il y a deux mani√®res de s√©lectionner une colonne : avec `pl.col('colname')` ou `pl.col.colname`.
Ce sera selon votre go√ªt :)

- **In√©galit√©s** : On peut vouloir garder seulement les lignes qui respectent une certaine condition.

Exemple, filtrer les lignes o√π la valeur de la colonne `var1` est sup√©rieure √† 0 :

```{python}
df.filter(pl.col('var1') >= 0)
```


- **Appartenance avec `is_in`** : Si on veut filtrer les donn√©es bas√©es sur une liste de valeurs possibles, la m√©thode `is_in` est tr√®s utile.

Exemple, pour garder uniquement les lignes o√π la colonne `experiment` a des valeurs 'test' ou 'validation' :

```{python}
df.filter(pl.col('experiment').is_in(['train', 'validation']))
```

Ces m√©thodes peuvent √™tre combin√©es pour cr√©er des conditions plus complexes.
Il est aussi possible d'utiliser les op√©rateurs logiques (`&` pour "et", `|` pour "ou") pour combiner plusieurs conditions.
Par d√©faut, les filtres sont consid√©r√©s comme des "et" : filter(condA, condB) renverra quand condA et condB sont vraies.
**Attention √† ne pas oublier les parenth√®ses pour d√©limiter les conditions !**

Exemple :
- s√©lectionner les lignes o√π `var1` est sup√©rieur √† 0 **et** `experiment` est √©gal √† 'test' ou 'validation':

```{python}
df.filter((pl.col("var1") >= 0) & pl.col("experiment").is_in(['train', 'validation']))
# √©quivalent √†
df.filter(pl.col("var1") >= 0, pl.col("experiment").is_in(['train', 'validation']))
```

- s√©lectionner les lignes o√π `var1` est sup√©rieur √† 0 **ou** `experiment` est √©gal √† 'test' ou 'validation':

```{python}
df.filter((pl.col("var1") >= 0) | pl.col("experiment").is_in(['train', 'validation']))
```

## Explorer des donn√©es tabulaires

En statistique publique, le point de d√©part n'est g√©n√©ralement pas la g√©n√©ration manuelle de donn√©es, mais plut√¥t des fichiers tabulaires pr√©existants.
Ces fichiers, qu'ils soient issus d'enqu√™tes, de bases administratives ou d'autres sources, constituent la mati√®re premi√®re pour toute analyse ult√©rieure.
Polars offre des outils puissants et tr√®s rapides pour importer ces fichiers tabulaires et les explorer en vue de manipulations plus pouss√©es.

### Importer et exporter des donn√©es

Polars est tr√®s sensible au format des donn√©es, l'import peut donc parfois √™tre un peu long √† d√©buger quand il n'arrive pas √† d√©terminer le format de donn√©es de la colonne.
En effet, Polars va traiter chaque colonne comme une s√©rie d'un des formats qu'il conna√Æt.
Les formats de donn√©es sont d√©taill√©s dans la [documentation de Polars](https://docs.pola.rs/api/python/stable/reference/datatypes.html).
Ils comprennent principalement:

| Famille de format | Format Polars | Remarques |
| -- | --¬†| -- |
| Caract√®res | - `pl.String` <br> - `pl.Utf8` | les donn√©es cat√©gorielles (`pl.Categorical`) sont encore peu matures dans Polars. <br> Il peut donc mieux valoir les traiter simplement comme des caract√®res usuels. |
| Nombres | | |
|    - entiers | - `pl.Int8` <br> - `pl.Int16` <br> - `pl.Int32` ... | |
|    - num√©rique | - `pl.Float16` <br> - `pl.Float32` <br> - `pl.Float64`  | |
| Temporelles | - `pl.Date` <br> - `pl.Datetime` <br> - `pl.Duration` ... | |
| Structures imbriqu√©es   | | - le format `pl.Struc` tr√®s pratique pour lire des formats imbriqu√©s comme des JSON |
| Valeurs manquantes   | | Polars traite les valeurs manquantes √† part. |

::: {.callout-caution}
A noter que pour Polars, les types `null` et `nan` sont diff√©rents.
`null` est une valeur manquante, quelque soit le type de valeur (caract√®re, num√©rique, temporelle,
imbriqu√©e ...). On le d√©clare avec un `None`.
`NaN`, qui signifie _Not a Number_, est une valeur manquante **num√©rique** (et non pour les entiers).
Cela signifie que Polars le traite comme un nombre particulier.
On d√©clare un `NaN` soit en utilisant Numpy `np.nan`, soit une syntaxe de Polars : `float("nan")`.

Plus de d√©tail sur la [documentation](https://docs.pola.rs/user-guide/expressions/missing-data/#null-and-nan-values).
:::

On peut convertir une colonne d'un type √† l'autre avec la m√©thode `cast` en indiquant
dans un dictionnaire le nom de la colonne et le format Polars souhait√©.

Par exemple, dans la table `df` d√©finie ci-dessus, Polars a assign√© les formats suivants :

```{python}
df.schema
```

On peut convertir `var1` en caract√®re et `date` en un format temporel par exemple :

```{python}
df.cast({"var1":pl.String, "date":pl.Date})
```

#### Importer un fichier CSV

Comme nous l'avons vu dans un pr√©c√©dent TP, le format CSV est l'un des formats les plus courants pour stocker des donn√©es tabulaires. Nous avons pr√©c√©demment utilis√© la librairie `csv` pour les manipuler comme des fichiers texte, mais ce n'√©tait pas tr√®s pratique. Pour rappel, la syntaxe pour lire un fichier CSV et afficher les premi√®res lignes √©tait la suivante :

```{python}
import csv

rows = []

with open("data/departement2021.csv") as file_in:
    csv_reader = csv.reader(file_in)
    for row in csv_reader:
        rows.append(row)

rows[:5]
```

Avec Polars, il suffit d'utiliser la fonction `read_csv()` pour importer le fichier comme un DataFrame, puis la fonction `head()`.

```{python}
df_departements = pl.read_csv('data/departement2021.csv')
df_departements.head()
```

A la diff√©rence de Pandas et √† la date d'√©criture de ce tutoriel, Polars ne charge pas directement depuis une URL et ne d√©zippe pas encore automatiquement le fichier csv (_cf._ [discussion de cette probl√©matique sur Github](https://github.com/pola-rs/polars/issues/19447)).
Soit on utilise la fonction idoine de `pandas` et apr√®s on transforme le dataframe en `polars` facilement gr√¢ce √† la fonction `pl.from_pandas`.

```{python}
url = "https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip"
pl.from_pandas(pd.read_csv(url, delimiter=";"))
```

Si l'on veut √©viter une d√©pendance √† `pandas` juste pour cette op√©ration, il faut donc faire ces √©tapes soi-m√™me pour le moment gr√¢ce aux packages `requests`et `zipfile`.
Prenons l'exemple d'un fichier CSV disponible sur le site de l'INSEE : le fichier des pr√©noms, issu des donn√©es de l'√©tat civil.

```{python}
# T√©l√©chargement et stockage du nom du fichier
url = "https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip"
file_Path = url.split('/')[-1]

response = requests.get(url)
if response.status_code == 200:
    with open(file_Path, 'wb') as file:
        file.write(response.content)

# zipfile permet de d√©zipper les fichiers
from zipfile import ZipFile

# on charge le fichier t√©l√©charger et extraction dans un dossier temp/
with ZipFile(file_Path, 'r') as zObject:
    zObject.extractall(path="temp/")

```

Par exemple, une erreur sera caus√©e ici √† cause de types de donn√©es incoh√©rentes d√©tect√©es par Polars.
En regardant le message d'erreur, Polars traite la colonne `anais` comme un entier.
Mais la lecture de la valeur `XXXX` cause une erreur et n'est pas convertible en entier par Polars.

```{python}
df_prenoms_url = pl.read_csv('temp/'+os.listdir('temp')[0], separator=";")
```

Dans ce cas, deux solutions existent :
- On peut dire √† Polars de traiter toutes les colonnes comme des caract√®res avec infer_schema=False

```{python}
df_prenoms_url = pl.read_csv('temp/'+os.listdir('temp')[0], separator=";", infer_schema=False)
df_prenoms_url
```

- On peut aussi lui dire de traiter la valeur `XXXX` comme une valeur manquante.
Polars va alors assigner des types √† chaque colonne qui ne seront pas toutes des caract√®res.

```{python}
df_prenoms_url = pl.read_csv('temp/'+os.listdir('temp')[0], separator=";", null_values=['XXXX'])
df_prenoms_url
```

Lorsqu'on travaille avec des fichiers CSV, il y a de nombreux arguments optionnels disponibles dans la fonction `read_csv()` qui permettent d'ajuster le processus d'importation en fonction des sp√©cificit√©s du fichier.
Ces arguments peuvent notamment permettre de d√©finir un d√©limiteur sp√©cifique (comme ci-dessus pour le fichier des pr√©noms), de sauter certaines lignes en d√©but de fichier, ou encore de d√©finir les types de donn√©es pour chaque colonne, et bien d'autres.
Tous ces param√®tres et leur utilisation sont d√©taill√©s dans la [documentation officielle](https://docs.pola.rs/api/python/stable/reference/api/polars.read_csv.html#polars.read_csv).

```{python}
# Pour nettoyer les fichiers
!rm $file_Path
!rm temp -rf
```


#### Exporter au format CSV

Une fois que les donn√©es ont √©t√© trait√©es et modifi√©es au sein de Polars, il est courant de vouloir exporter le r√©sultat sous forme de fichier CSV pour le partager, l'archiver ou l'utiliser dans d'autres outils.
Polars offre une m√©thode simple pour cette op√©ration : `write_csv()`.
Supposons par exemple que l'on souhaite exporter les donn√©es du DataFrame `df_departements` sp√©cifiques aux cinq d√©partements d'outre-mer.

```{python}
df_departements_dom = df_departements.filter(pl.col("DEP").is_in(["971", "972", "973", "974", "975"]))
df_departements_dom.write_csv('output/departements2021_dom.csv', separator=";")
```


#### Importer un fichier Parquet

Le format Parquet est un autre format pour le stockage de donn√©es tabulaires, de plus en plus fr√©quemment utilis√©.
Sans entrer dans les d√©tails techniques, le format Parquet pr√©sente diff√©rentes caract√©ristiques qui en font un choix privil√©gi√© pour le stockage et le traitement de gros volumes de donn√©es.
En raison de ces avantages, ce format est de plus en plus utilis√© pour la mise √† disposition de donn√©es √† l'Insee.
Il est donc essentiel de savoir importer et requ√™ter des fichiers Parquet avec Polars.

Importer un fichier Parquet dans un DataFrame Pandas se fait tout aussi facilement que pour un fichier CSV.
La fonction se nomme `read_parquet()`.

```{python}
df_departements = pl.read_parquet('data/departement2021.parquet')
df_departements.head()
```

#### Exporter au format Parquet

L√† encore, tout se passe comme dans le monde des CSV : on utilise la m√©thode `write_parquet()` pour exporter un DataFrame dans un fichier Parquet.

```{python}
df_departements_dom = df_departements.filter(pl.col("DEP").is_in(["971", "972", "973", "974", "975"]))
df_departements_dom.write_parquet('output/departements2021_dom.parquet')
```

Une des grandes forces du format Parquet, en comparaison des formats texte comme le CSV, est sa capacit√© √† stocker des m√©ta-donn√©es, i.e. des donn√©es permettant de mieux comprendre les donn√©es contenues dans le fichier.
En particulier, un fichier Parquet inclut dans ses m√©ta-donn√©es le sch√©ma des donn√©es (noms des variables, types des variables, etc.), ce qui en fait un format tr√®s adapt√© √† la diffusion de donn√©es.
V√©rifions ce comportement en reprenant le DataFrame que nous avons d√©fini pr√©c√©demment.

```{python}
df = pl.DataFrame(
    data = {
        "var1": [1.3, 5.6, float("nan"), float("nan"), 0, float("nan")],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    },
    schema_overrides={
        "experiment": pl.Categorical,
        "date": pl.Date
        }
)

```

On utilise cette fois deux types de donn√©es sp√©cifiques, pour les donn√©es cat√©gorielles (`categorical`) et pour les donn√©es temporelles (`date`), comme pr√©sent√© pr√©c√©demment.
Le format des donn√©es est imprim√© par Polars dans la sortie : on voit maintenant en
dessous du nom des colonnes que le format des colonnes `experiment` et `date` ont chang√©.
On verra plus loin dans le tutoriel comment utiliser ces types.

```{python}
df
```

V√©rifions √† pr√©sent que l'export et le r√©-import de ces donn√©es en Parquet pr√©serve le sch√©ma.

```{python}
df.write_parquet("output/df_test_schema.parquet")
df_test_schema_parquet = pl.read_parquet('output/df_test_schema.parquet')

df_test_schema_parquet
```

A l'inverse, un fichier CSV ne contenant par d√©finition que du texte, ne permet pas de pr√©server ces donn√©es.
Les variables dont nous avons sp√©cifi√© le type sont import√©es comme des strings (type `object` en Polars).

```{python}
df.write_csv("output/df_test_schema.csv")
df_test_schema_csv = pl.read_csv('output/df_test_schema.csv')

df_test_schema_csv
```

#### Importer depuis un dictionnaire

Comme illustr√© dans le tutoriel [Travailler avec des fichiers CSV et JSON](../csv-json-files/tutorial.qmd), on peut aussi cr√©er un dataFrame Polars √† partir d'un dictionnaire.
Cela sera tr√®s utile par exemple quand on r√©cup√®re les donn√©es d'une API (_cf._ cours de
[Python pour la data-science](https://pythonds.linogaliana.fr/) et la partie sur les API).

Les m√©thodes `from_dict` et `from_dicts` permettent de cr√©er des DataFrames √† partir de dictionnaires.

- `from_dict` prend comme argument un **dictionnaire des colonnes**, de mani√®re similaire √† `pl.DataFrame` :
on va √©crire par exemple `pl.from_dict({"a": [1, 2], "b": [3, 4]})`
- `from_dicts` par contre prend comme argument une **liste des lignes** : on va √©crire par exemple `pl.from_dicts([{"a": 1, "b": 4}, {"a": 2, "b": 5}, {"a": 3, "b": 6}])`

```{python}
import requests
import json

response = requests.get("https://data.geopf.fr/geocodage/search?q=comedie&type=street")
```

Le r√©sultat de la requ√™te est un dictionnaire avec 3 cl√©s, dont la deuxi√®me, 'features', stocke les r√©sultats sous forme d'une liste.
Pour importer directement ce fichier en Polars, on peut utiliser la m√©thode from_dict ou from_dicts.

```{python}
df = pl.from_dicts(json.loads(response.text))
df.head(2)
```

On retrouve dans le DataFrame les trois colonnes dont la cl√© repr√©sente le nom de la colonne et la valeur dans le dictionnaire la valeur dans la colonne.
La premi√®re et derni√®re colonnes sont des valeurs constantes au format `pl.String` et les r√©sultats sont stock√©s dans la colonne `features`.
Cette derni√®re est au format `pl.Struct`, qui fonctionne de mani√®re similaire √† un dictionnaire.

On peut facilement r√©cup√©rer le d√©tail des r√©sultats gr√¢ce √† la m√©thode `unnest` :

```{python}
df = df.select("features").unnest("features")
df
```

#### Autres exports et imports

- **Exporter vers un dictionnaire**:

De mani√®re similaire, Polars permet d'exporter un DataFrame vers un dictionnaire.

    - La m√©thode `to_dict` permet d'exporter un DataFrame "en format colonne".
    L'export sera un seul dictionnaire dont les cl√©s sont les noms des colonnes et
    les valeurs les listes des valeurs prises par colonne `{'a':[1, 2, 3], 'b': [3, 2, 1]}`.
    - La m√©thode `to_dicts`permet d'exporter le DataFrame "en format ligne".
    L'export sera une liste dont les termes seront les lignes du DataFrame,
    repr√©sent√©es √† chaque fois par un dictionnaire : `[{'a': 1, 'b': 3}, {'a': 2, 'b': 2}, {'a': 3, 'b': 1}]`

- **Depuis/vers Pandas**:
Polars permet aussi facilement d'exporter et d'importer des DataFrame au format Pandas.
Cela permet de travailler avec Polars et de convertir un DataFrame en Pandas quand un package n'autorise pas l'import depuis Polars.
Il suffit d'utiliser la fonction `pl.from_pandas(df)` et la m√©thode `df.to_pandas()`.

### Visualiser un √©chantillon des donn√©es

Lorsqu'on travaille avec des jeux de donn√©es volumineux, il est souvent utile de visualiser rapidement un √©chantillon
des donn√©es pour avoir une id√©e de leur structure, de leur format ou encore pour d√©tecter d'√©ventuels probl√®mes.
Polars offre plusieurs m√©thodes pour cela.

Par d√©faut, Polars indique les cinq premi√®res et cinq derni√®res lignes d'un DataFrame et indique la forme (_shape_).

```{python}
df_departements
```

La m√©thode `head()` permet d'afficher les premi√®res lignes du DataFrame.
Par d√©faut, elle retourne les 5 premi√®res lignes, mais on peut sp√©cifier un autre nombre en argument si n√©cessaire.

```{python}
df_departements.head()
```

```{python}
df_departements.head(10)
```

√Ä l'inverse, la m√©thode `tail()` donne un aper√ßu des derni√®res lignes du DataFrame.

```{python}
df_departements.tail()
```

L'affichage des premi√®res ou derni√®res lignes peut parfois ne pas √™tre repr√©sentatif de l'ensemble du jeu de donn√©es, lorsque les donn√©es sont tri√©es par exemple. Afin de minimiser le risque d'obtenir un aper√ßu biais√© des donn√©es, on peut utiliser la m√©thode `sample()`, qui s√©lectionne un un √©chantillon al√©atoire de lignes. Par d√©faut, elle retourne une seule ligne, mais on peut demander un nombre sp√©cifique de lignes en utilisant l'argument `n`.

```{python}
df_departements.sample(n=5)
```

### Obtenir une vue d'ensemble des donn√©es

L'une des premi√®res √©tapes lors de l'exploration de nouvelles donn√©es est de comprendre la structure g√©n√©rale du jeu de donn√©es.
La m√©thode `glimpse()` de Polars offre une vue d'ensemble rapide des donn√©es, notamment en termes de types de donn√©es et affiche les donn√©es.

```{python}
df.glimpse()
```

Plusieurs √©l√©ments d'information cl√©s peuvent √™tre extraits de ce r√©sultat :

- **Taille de la table** : le nombre de ligne et de colonnes sont indiqu√©es

- **sch√©ma** : la liste des colonnes est affich√©e avec des informations tr√®s utiles sur le sch√©ma des donn√©es :

  - **Format** : Le format Polars de donn√©es de la colonne, par exemple `<str>`pour un format caract√®re (ou `pl.String`).
  Cela permet de comprendre la nature des informations stock√©es dans chaque colonne.

  - **les premi√®res valeurs** : apr√®s chaque colonne sont indiqu√©es, √† l'horizontale, les premi√®res valeurs de la colonne.
  Cela permet de mieux comprendre le sch√©ma du DataFrame.

Contrairement √† Pandas, cette m√©thode n'affiche pas les valeurs manquantes.
Pour cela, il faut utiliser les statistiques descriptives.

### Calculer des statistiques descriptives

En compl√©ment des informations renvoy√©es par la m√©thode `glimpse()`, on peut vouloir obtenir des
statistiques descriptives simples afin de visualiser rapidement les distributions des variables.
La m√©thode `describe()` permet d'avoir une vue synth√©tique de la distribution des donn√©es dans chaque colonne.

```{python}
df.describe()
```

Notons que, l√† encore, la variable `count` renvoie le nombre de valeurs **non-manquantes** dans chaque variable.



## Principales manipulations de donn√©es

### Transformer les donn√©es

Les op√©rations de transformation sur les donn√©es sont essentielles pour fa√ßonner, nettoyer et pr√©parer les donn√©es en vue de leur analyse.
Les transformations peuvent concerner l'ensemble du DataFrame, des colonnes sp√©cifiques ou encore des lignes sp√©cifiques.

#### Transformer les colonnes

En √©quivalent de `mutate` du tidyverse ou du `replace` de pandas, Polars utilise `with_columns`.
Le fonctionnement est similaire au `mutate` du tidyverse.
Il permet ainsi de modifier des valeurs de colonne, d'effectuer des op√©rations etc.



La fonction with_columns fonctionne de  deux mani√®res diff√©rentes :

    1. en utilisant une syntaxe plus proche du SQL et de Python, gr√¢ce √† la m√©thode `.alias()`.
    2. plus similaire au tidyverse, en assignant les variables avec une √©quation

Par exemple, si l'on veut d√©finir la colonne `b` comme la colonne `a` divis√©e par 2 , on va utiliser la formule suivante :

```{python}
df = pl.DataFrame({'a':np.random.randint(-10, 10, 6), 'b': np.random.randint(5, 15, 6)})

df.with_columns(
    # Premi√®re m√©thode :
    (pl.col("a")/2).alias("b"),
    (np.abs(pl.col("a"))).alias("abs_a"),
    # Ou l'autre m√©thode :
    b2=pl.col("a")/2,
    abs_a2=np.abs(pl.col("a"))
)

```


::: {.callout-tip}
La structure `pl.when` est tr√®s pratique et √©quivaut √† la structure `ifelse` en R.

Par exemple, si je veux d√©finir la colonne val √† partir de la colonne foo selon le sch√©ma : si foo > 2 : val = 1, sinon val = 1 + bar, cela s'√©crite en polars :


```{python}
df = pl.DataFrame({'foo':np.random.randint(-10, 10, 6), 'bar': np.random.randint(5, 15, 6)})
df.with_columns(
    pl.when(pl.col('foo') > 2).then(1).otherwise(1 + pl.col('bar')).alias("val")
)
```

:::

::: {.callout-warning}

Attention, en Polars, les caract√®res simples sont assimil√©s aux  colonnes.
Pour ins√©rer un caract√®re, utiliser pl.lit.

```{python}
df = pl.DataFrame({'foo':np.random.randint(-10, 10, 6), 'bar': np.random.randint(5, 15, 6)})
df.with_columns(
    val="foo",
    this_is_foo=pl.lit("foo")
)
```

:::

La m√©thodes fill_null permet de remplacer les valeurs manquante

```{python}
df.fill_null(strategy='zero')
```


Mais il existe d'autres transformations que l'on applique g√©n√©ralement au niveau d'une ou de quelques colonnes. Par exemple, lorsque le sch√©ma n'a pas √©t√© bien reconnu √† l'import, il peut arriver que des variables num√©riques soient d√©finies comme des string.

```{python}
df = pl.DataFrame(
    data = {
        "var1": [1.3, 5.6, None],
        "var2": ["1", "5", "18"],
    }
)

df.glimpse()
```

Dans ce cas, on peut utiliser la m√©thode `cast` pour convertir la colonne dans le type souhait√© en sp√©cifiant avec un dictionnaire les colonnes √† modifier.
Le dictionnaire est √©crit sous le format `{'nom de la colonne 1':'type Polars √† appliquer', 'nom de la colonne 2':'type Polars √† appliquer'}`.
Les types sont d√©crits plus haut et dans la [documentation de Polars](https://docs.pola.rs/api/python/stable/reference/datatypes.html).

```{python}
df = df.cast({'var2':pl.Int32})

df.glimpse()
```

#### Renommer des colonnes

Une autre op√©ration fr√©quente est le renommage d'une ou plusieurs colonnes. Pour cela, on peut utiliser la m√©thode [rename()](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.rename.html#polars.DataFrame.rename), √† laquelle on passe un dictionnaire qui contient autant de couples cl√©-valeur que de variables √† renommer, et dans lequel chaque couple cl√©-valeur est de la forme `'ancien_nom': 'nouveau_nom'`.

```{python}
df.rename({'var2': 'age'})
```


#### Fonctions complexes

N√©anmoins, on peut se retrouver dans certains (rares) cas o√π une op√©ration ne peut pas √™tre facilement vectoris√©e ou o√π la logique est complexe.
Supposons par exemple que l'on souhaite calculer la variation annuelle du PIB par pays dans une table comportant les pays et les ann√©es.

```{python}
# Donn√©es fictives de PIB par pays
df = pl.DataFrame({
    'country': ['France', 'France', 'France', 'Germany', 'Germany', 'Germany', 'UK', 'UK', 'UK'],
    'year': [2019, 2020, 2021, 2019, 2020, 2021, 2019, 2020, 2021],
    'gdp': [21.43, 20.21, 22.54, 1.73, 1.66, 1.78, 2.83, 2.71, 2.99]
})

# On d√©finit une fonction dite User-defined-function (UDF) pour calculer la croissance par pays
def calculate_percentage_change(group):
    group = group.sort('year')
    group = group.with_columns(
        ((pl.col('gdp') - pl.col('gdp').shift(1)) / pl.col('gdp').shift(1) * 100).alias('percentage_change')
    )
    return group

# On applique la fonction en utilisant map_groups
print(df.group_by('country').map_groups(calculate_percentage_change))

```


### Trier les valeurs

Le tri des donn√©es est particuli√®rement utile pour l'exploration et la visualisation de donn√©es. Avec Polars, on utilise la m√©thode [sort()](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.sort.html#polars.DataFrame.sort) pour trier les valeurs d'un DataFrame selon une ou plusieurs colonnes.

```{python}
df = pl.DataFrame(
    data = {
        "var1": [1, 9, 9, 14, 13],
        "var2": [3, 7, 11, 15, 4],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-04-02"],
    }
)

df
```

Pour trier les valeurs selon une seule colonne, il suffit de passer le nom de la colonne en param√®tre.

```{python}
df.sort(by='var1')
```

Par d√©faut, le tri est effectu√© dans l'ordre croissant. Pour trier les valeurs dans un ordre d√©croissant, il suffit de param√©trer `descending=True`.

```{python}
df.sort(by='var1', descending=True)
```

Si on souhaite trier le DataFrame sur plusieurs colonnes, on peut fournir une liste de noms de colonnes. On peut √©galement choisir de trier de mani√®re croissante pour certaines colonnes et d√©croissante pour d'autres en sp√©cifiant les bool√©ns dans une liste.

```{python}
df.sort(by=['var1', 'var2'], descending=[True, True])
```


### Agr√©ger des donn√©es

L'agr√©gation des donn√©es est un processus dans lequel les donn√©es vont √™tre ventil√©es en groupes selon certains crit√®res, puis agr√©g√©es selon une fonction d'agr√©gation appliqu√©e ind√©pendamment √† chaque groupe. Cette op√©ration est courante lors de l'analyse exploratoire ou lors du pr√©traitement des donn√©es pour la visualisation ou la mod√©lisation statistique.

```{python}
df = pl.DataFrame(
    data = {
        "var1": [1.3, 5.6, None, None, 0, None],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df.head()
```

#### L'op√©ration `group_by`

La m√©thode `group_by` de Polars permet de diviser le DataFrame en sous-ensembles selon les valeurs d'une ou plusieurs colonnes, puis d'appliquer une fonction d'agr√©gation √† chaque sous-ensemble. Elle renvoie un objet de type `dataframe.group_by` qui ne pr√©sente pas de grand int√©r√™t en soi, mais constitue l'√©tape interm√©diaire indispensable pour pouvoir ensuite appliquer une ou plusieurs fonction(s) d'agr√©gation aux diff√©rents groupes.

```{python}
df.group_by('experiment')
```

#### Fonctions d'agr√©gation

Une fois les donn√©es group√©es, on peut appliquer des fonctions d'agr√©gation pour obtenir un r√©sum√© statistique. Polars int√®gre un certain nombre de ces fonctions qui  fonctionnent sur la table de donn√©e enti√®re, *cf.* [liste compl√®te dans la documentation](https://docs.pola.rs/api/python/stable/reference/dataframe/aggregation.html) ou sur des sous-ensembles de la table, en utilisant `group_by`, *cf.* [liste compl√®te dans la documentation](https://docs.pola.rs/api/python/stable/reference/dataframe/group_by.html).


Voici quelques exemples d'utilisation de ces m√©thodes avec `group_by`.

Par d√©faut, Polars appliquera les fonctions usuelles √† l'ensemble des colonnes.
Pour sp√©cifier une colonne, il faut utiliser la fonction `agg`, d√©taill√©e plus bas.

- Par exemple, compter le nombre d'occurrences dans chaque groupe.

```{python}
df.group_by('experiment').count()
```

- Calculer la somme d'une variable par groupe.

```{python}
df.group_by('experiment').agg(pl.col('var2').sum())
```

::: {.callout-tip}
Polars a aussi introduit des wrappers utiles pour les fonctions principales d'aggregation sum, std, count, quantile, cum_count, cum_sum, max, min, median, sqrt.

Pour ces fonctions, on peut les appeler directement en faisant `pl.sum('var2')` et non `pl.col('var2').sum()`.

Par exemple :
```{.python}
df.group_by('experiment').agg(pl.sum('var2'))
```

est identique √† :
```{.python}
df.group_by('experiment').agg(pl.col('var2').sum())
```

:::

- Ou encore compter le nombre de valeurs unique d'une variable par groupe. Les possibilit√©s sont nombreuses.

```{python}
# Pour le nombre de valeurs uniques dans chaque groupe
df.group_by('experiment').agg(pl.col('var2').n_unique())
```

Lorsqu'on souhaite appliquer plusieurs fonctions d'agr√©gation √† la fois ou des fonctions personnalis√©es, on utilise la m√©thode `agg`. Cette m√©thode fonctionne selon une structure similaire √† `with_columns`. Elle permet de d√©finir plus finement les fonctions d'agr√©gation.

```{python}
df.group_by('experiment').agg(
    pl.col('var1').mean().alias('mean_var1'),
    pl.col('var2').count().alias('count_var2')
)
```

::: {.callout-tip}
Au lieu d'utiliser la m√©thode alias, on peut utiliser l'attribut `name` de Polars qui renvoie le nom de la colonne et les fonctions associ√©es. Plus de d√©tails sont pr√©sent√©s dans la [documentation officielle](https://docs.pola.rs/api/python/stable/reference/expressions/name.html).

Par exemple, d√©finir `.alias('mean_var1')` est l'√©quivalent de `.name.prefix('mean_')`
:::

::: {.callout-note title="Le cha√Ænage de m√©thodes"}
Les exemples pr√©c√©dents illustrent un concept important en Polars : le cha√Ænage de m√©thodes. Ce terme d√©signe la possibilit√© d'encha√Æner les transformations appliqu√©es √† un DataFrame en lui appliquant √† la cha√Æne des m√©thodes, comme le `pipe` du tidyverse. A chaque m√©thode appliqu√©e, un DataFrame interm√©diaire est cr√©√© (mais non assign√© √† une variable), qui devient l'input de la m√©thode suivante.

Le cha√Ænage de m√©thodes permet de combiner plusieurs op√©rations en une seule expression de code. Cela peut am√©liorer l'efficacit√© en √©vitant les assignations interm√©diaires et en rendant le code plus fluide et plus facile √† lire.
Cela favorise √©galement un style de programmation fonctionnel o√π les donn√©es passent √† travers une cha√Æne de transformations de mani√®re fluide.

```{.python}
subset_df = (
    df
    .filter(pl.col('sample') == "sample1", pl.col("date") >= '2022-01-02')
    .drop('sample')
    .glimpse()
)
```

:::

### Traiter les valeurs manquantes

Les valeurs manquantes sont une r√©alit√© courante dans le traitement des donn√©es r√©elles et peuvent survenir pour diverses raisons, telles que des non-r√©ponses √† un questionnaire, des erreurs de saisie, des pertes de donn√©es lors de la transmission ou simplement parce que l'information n'est pas applicable. Pandas offre plusieurs outils pour g√©rer les valeurs manquantes.

#### Repr√©sentation des valeurs manquantes

On peut tout d'abord traiter les valeurs manquantes avec les m√©thodes associ√©es.
Pour rappel, en Polars, les valeurs `NaN` et `null` sont diff√©rentes, alors qu'en Pandas elles sont trait√©es de mani√®re similaire.

| | **Type `null`** | **Type `NaN`** |
|--|--|--|
| **Signification** | Seule `Null` est consid√©r√© par d√©faut comme une valeur manquante et est l'unique type de valeur manquante dans Polars (que le type original soit temporel, num√©rique, un charact√®re ...). | `NaN` repr√©sente √† l'inverse des valeurs num√©riques erron√©es, comme le r√©sultat d'une division par 0. |
| **Comment les cr√©er ?** | Les valeurs manquantes peuvent √™tre cr√©es simplement avec `None`. | Pour cr√©er une valeur `NaN`, valable uniquement pour les types num√©riques, on peut utiliser `np.nan` de NumPy ou `float("nan")` de Polars. |
| Comportement dans Polars | Ces valeurs sont en g√©n√©ral **ignor√©es**. | Ces valeurs **se propagent**. |


V√©rifions cette propri√©t√©.
Pour identifier o√π se trouvent les valeurs manquantes, on utilise la fonction `is_null()` ou `is_nan()` appliqu√©e √† une colonne.

```{python}
df = pl.DataFrame(
    data = {
        "var1": [1.3, 5.6, 2.2, 1.4, None, 1.7],
        "var2": np.append(np.random.randint(-10, 10, 5), np.nan),
        "experiment": ["test", "train", "test", None, "train", "validation"],
        "sample": "sample1"
    }
)
df.with_columns(
    var1_na=pl.col("var1").is_nan(),
    var1_null=pl.col("var1").is_null()
)
```

On peut √† l'inverse √©liminer les lignes qui comportent une valeur manquante ou un `NaN` :

```{python}
print(df.drop_nans())
print(df.drop_nulls())
```

#### Calculs sur des colonnes contenant des valeurs manquantes

Lors de calculs statistiques, les valeurs nulles sont g√©n√©ralement ignor√©es en Polars. Par exemple, la m√©thode `.mean()` calcule la moyenne des valeurs non manquantes.

```{python}
df['var1'].mean()
```

En revanche, les valeurs 'Not a Number', ou `NaN`, ont plut√¥t tendance √† se propager.

```{python}
df['var2'].mean()
```

#### Suppression des valeurs manquantes

Les m√©thodes `drop_nulls()` et `drop_nans()` permettent de supprimer les lignes contenant des valeurs manquantes ou des NaN.

Par d√©faut, toute ligne contenant au moins une valeur manquante ou NaN est supprim√©e.

```{python}
df.drop_nans()
```

Si l'on sp√©cifie une liste de colonnes, seules les lignes comportant une valeur manquante ou NaN de cette colonne seront supprim√©es.

```{python}
df.drop_nulls(['var1'])
```

#### Remplacement des valeurs manquantes

Pour g√©rer les valeurs manquantes dans un DataFrame, une approche commune est l'imputation, qui consiste √† remplacer les valeurs manquantes par d'autres valeurs. La m√©thode [`fill_null()`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.fill_null.html) permet d'effectuer cette op√©ration de diff√©rentes mani√®res dans toute la table de donn√©e.

- Une premi√®re possibilit√© est le remplacement par une valeur constante.

```{python}
df.fill_null(value=0)
```

::: {.callout-warning title="Changement de repr√©sentation des valeurs manquantes"}
Il peut parfois √™tre tentant de changer la manifestation d'une valeur manquante pour des raisons de visibilit√©, par exemple en la rempla√ßant par une cha√Æne de caract√®res :

```{python}
df.fill_null(value="MISSING")
```

En pratique, cette fa√ßon de faire n'est pas recommand√©e. Il est en effet pr√©f√©rable de conserver la convention standard de `Polars` (l'utilisation des `null`), d'abord pour des questions de standardisation des pratiques qui facilitent la lecture et la maintenance du code, mais √©galement parce que la convention standard est optimis√©e pour la performance et les calculs √† partir de donn√©es contenant des valeurs manquantes.
:::

La m√©thide `fill_null()` de Polars propose 4 m√©thodes pr√©-d√©finies pour remplacer les valeurs manquantes par l'argument `strategy`:
- toute valeur constante indiqu√©e (`strategy=None`, argument par d√©faut);
- soit des 0, soit des 1 (`strategy="zero"`, `strategy="one"` :);
- en reportant la valeur pr√©c√©dente (`strategy=‚Äòforward‚Äô`) ou la valeur suivante (`strategy=‚Äòbackward‚Äô`) de la colonne ;
- en indiquant le minimum, le maximum ou la m√©diane de la colonne (`strategy=‚Äòmin‚Äô`, `strategy=‚Äòmax‚Äô`, `strategy=‚Äòmean‚Äô`).

Si l'on veut remplacer uniquement les valeurs manquantantes d'une colonne en Polars, il est plus utile d'utiliser la m√©thode `with_columns()` en utilisant l'agilit√© des s√©lecteurs de Polars.

```{python}
df.with_columns(
    pl.col(pl.NUMERIC_DTYPES).fill_null(strategy="mean")
)
```


::: {.callout-warning title="Biais d'imputation"}
Remplacer les valeurs manquantes par une valeur constante, telle que z√©ro, la moyenne ou la m√©diane, peut √™tre probl√©matique. Si les donn√©es ne sont pas manquantes au hasard (*Missing Not At Random* - *MNAR*), cela peut introduire un biais dans l'analyse. Les variables *MNAR* sont des variables dont la probabilit√© d'√™tre manquantes est li√©e √† leur propre valeur ou √† d'autres variables dans les donn√©es. Dans de tels cas, une imputation plus sophistiqu√©e peut √™tre n√©cessaire pour minimiser les distorsions. Nous en verrons un exemple en exercice de fin de tutoriel.
:::

De mani√®re similaire, la m√©thode `fill_nan()` permet de remplacer les valeurs num√©riques NaN par toute valeur indiqu√©e.

### Traiter les donn√©es de types sp√©cifiques

#### Donn√©es textuelles

Les donn√©es textuelles n√©cessitent souvent un nettoyage et une pr√©paration avant l'analyse.
Polars fournit via la librairie de m√©thodes `str` un ensemble d'op√©rations vectoris√©es qui rendent la pr√©paration des donn√©es textuelles √† la fois simple et tr√®s efficace.
La m√©thode `str` s'applique aux s√©ries Polars.
L√† encore, les possibilit√©s sont multiples et d√©taill√©es dans la [documentation](https://docs.pola.rs/api/python/stable/reference/series/string.html).
Nous pr√©sentons ici les m√©thodes les plus fr√©quemment utilis√©es dans l'analyse de donn√©es.

```{python}
df = pl.DataFrame(
    data = {
        "var1": [1.3, 5.6, None, None, 0, None],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "test", "train", "validation"],
        "sample": ["  sample1", "sample1  ", " sample2 ", "   sample2   ", "sample2  ", "sample1"]
    }
)

df
```

Une premi√®re op√©ration fr√©quente consiste √† extraire certains caract√®res d'une cha√Æne.
Par exemple, si l'on veut extraire le dernier caract√®re de la variable `sample` afin de ne retenir que le chiffre de l'√©chantillon.
Un petit tour par la [documentation](https://docs.pola.rs/api/python/stable/reference/series/string.html) montre que la m√©thode `tail()` est notre gagnant.

```{python}
df.with_columns(
    sample_n = pl.col('sample').str.tail(1)
)

```

Le principe √©tait le bon, mais la pr√©sence d'espaces superflus dans nos donn√©es textuelles (qui ne se voyaient pas √† la visualisation du DataFrame !) a rendu l'op√©ration plus difficile que pr√©vue.
C'est l'occasion d'introduire la famille de m√©thode `strip` (`.str.strip_chars()`, `.str.strip_chars_start()` et `.str.strip_chars_end()`) qui respectivement retirent les espaces superflus (ou tout caract√®re indiqu√©) des deux c√¥t√©s ou d'un seul.

```{python}
(
    df
    .with_columns(
        sample = pl.col('sample').str.strip_chars()
        )
    .with_columns(
        sample_n = pl.col('sample').str.tail(1)
    )
)
```

::: {.callout-note}
Remarquez la pr√©sence de deux `with_columns()` et non d'un seul.
C'est parce que Polars √©tant optimis√©, il effectue les op√©rations sur `with_columns` en parall√®le.
Ainsi, si l'on ne le fait pas de mani√®re s√©quentielle, il ne prend pas en compte le fait qu'on enl√®ve les espace superflus dans 'sample' √† la premi√®re ligne avant d'en extraire le dernier caract√®re.
:::

On peut √©galement vouloir filtrer un DataFrame en fonction de la pr√©sence ou non d'une certaine cha√Æne (ou sous-cha√Æne) de caract√®res.
On utilise pour cela la m√©thode `.str.contains()`.

```{python}
df.filter(pl.col('experiment').str.contains('test'))
```

Enfin, on peut vouloir remplacer une cha√Æne (ou sous-cha√Æne) de caract√®res par une autre, ce que permet la m√©thode `str.replace()`.

```{python}
df.with_columns(
    experiment = pl.col('experiment').str.replace('validation', 'val')
)

```

#### Donn√©es cat√©gorielles

Les donn√©es cat√©gorielles sont des variables qui contiennent un nombre restreint de modalit√©s.
A l'instar de `R` avec la notion de `factor`, Polars a un type de donn√©es sp√©cial, `Categorical`, qui peut √™tre utilis√© pour repr√©senter des donn√©es cat√©gorielles.

Ce type de donn√©es est cependant d√©licat √† g√©rer en Polars et, en g√©n√©ral, on pr√©f√®rera travailler avec des donn√©es de type caract√®re.

En effet, les m√©thodes usuelles pour renommer des cat√©gories, dissocier son nom et sa repr√©sentation, r√©arranger l'ordre dans laquelle les cat√©gories sont visualis√©es sont encore tr√®s largement perfectibles (voire inexistantes :o).

Si l'on veut quand m√™me avoir la base, on peut convertir une variable au format `Categorical`, on peut directement utiliser la m√©thode `cast()`.
Polars va alors inf√©rer le sch√©ma de la cat√©gorie et le stocker dans un objet de type `Categories`.


```{python}
df = pl.DataFrame(
    data = {
        "var1": [1.3, 5.6, None, None, 0, None],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", None, "train", "validation"],
    }
)
print(df.dtypes)
```

```{python}
df = df.with_columns(pl.col("experiment").cast(pl.Categorical))
print(df.dtypes)
```

Les `Categories` ont un certain nombre de m√©thodes, tout comme les donn√©es textuelles, pr√©sent√©es dans la [documentation](https://docs.pola.rs/api/python/stable/reference/series/categories.html).

On pourra alors par exemple acc√©der aux cat√©gories avec la m√©thode `Series.cat.get_categories()`

```{python}
df['experiment'].cat.get_categories()
```


#### Donn√©es temporelles

Les donn√©es temporelles sont souvent pr√©sentes dans les donn√©es tabulaires afin d'identifier temporellement les observations recueillies.
Polars offre des fonctionnalit√©s pour manipuler ces types de donn√©es, notamment gr√¢ce au type `pl.Datetime` qui permet une manipulation pr√©cise des dates et des heures.

```{python}
df = pl.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04"],
        "sample": ["sample1", "sample1", "sample2", "sample2"]
    }
)

df.dtypes
```

Pour manipuler les donn√©es temporelles, il est n√©cessaire de convertir les cha√Ænes de caract√®res en colonne de type temporel sous Polars.
Il existe 4 types temporels de colonnes en Polars, comme rappel√© dans la [documentation](https://docs.pola.rs/api/python/stable/reference/datatypes.html):

- le type [`pl.Datetime`](https://docs.pola.rs/api/python/stable/reference/api/polars.datatypes.Date.html#polars.datatypes.Datetime) pour un format date et heure ;
- le type [`pl.Date`](https://docs.pola.rs/api/python/stable/reference/api/polars.datatypes.Date.html#polars.datatypes.Date) pour un format Date seul ;
- le type [`pl.Duration`](https://docs.pola.rs/api/python/stable/reference/api/polars.datatypes.Date.html#polars.datatypes.Duration) pour une dur√©e ;
- le type [`pl.Time`](https://docs.pola.rs/api/python/stable/reference/api/polars.datatypes.Date.html#polars.datatypes.Time) pour une heure.

Polars transforme des donn√©es textuelles en donn√©es temporelles fait via les m√©thodes [`str.to_date()`](https://docs.pola.rs/api/python/stable/reference/series/api/polars.Series.str.to_datetime.html#polars.Series.str.to_date), [`str.to_datetime()`](https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.str.to_datetime.html), [`str.to_time()`](https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.str.to_time.html), [`str.strptime()`](https://docs.pola.rs/api/python/stable/reference/expressions/api/polars.Expr.str.strptime.html).

Au cas d'esp√®ce, la m√©thode `str.to_date()` remplit parfaitement notre besoin.

```{python}
df = df.with_columns(
    date = pl.col('date').str.to_date("%Y-%m-%d")
)
df.dtypes
```

Une fois converties, les dates peuvent √™tre format√©es, compar√©es et utilis√©es dans des calculs.
Il faudra cependant bien penser √† traduire les dates d'un format textuel √† un format temporel.
Par exemple, pour filter des donn√©es, on devra sp√©cifier les bornes en appelant les fonctions d√©di√©es de Polars ou du module `datetime`.

```{python}
df.filter(pl.col('date') >= pl.date(2022, 1, 1), pl.col('date') < pl.date(2022, 1, 3))
```

```{python}
from datetime import datetime
df.filter(pl.col('date') >= datetime.strptime("2022-01-01", "%Y-%m-%d"), pl.col('date') < datetime.strptime("2022-01-03", "%Y-%m-%d"))
```

On peut √©galement vouloir r√©aliser des filtrages moins pr√©cis, faisant intervenir l'ann√©e ou le mois.
Polars comprend toute une s√©rie de m√©thode sur les donn√©es temporelles, pr√©sent√©es dans la [documentation](https://docs.pola.rs/api/python/stable/reference/series/temporal.html).
Elles permettent d'extraire facilement des composants sp√©cifiques de la date, comme l'ann√©e, le mois, le jour, l'heure, etc.

```{python}
df = df.with_columns(
    year = pl.col('date').dt.year(),
    month = pl.col('date').dt.month(),
    day = pl.col('date').dt.day()
)

df.filter(pl.col('year')==2022)

```

Dans ce cas pr√©cis, on peut utiliser plus finement les propri√©t√©s de la m√©thode `filter()` et avoir le m√™me r√©sultat en faisant directement `df.filter(pl.col('date').dt.year() == 2022)`.

Enfin, les calculs faisant intervenir des dates deviennent possible.
On peut ajouter ou soustraire des p√©riodes temporelles √† des dates, et les comparer entre elles.
Les fonctions utilis√©es sont issues de `Polars`, mais sont tr√®s semblables dans leur fonctionnement √† celles du module [datetime](https://docs.python.org/fr/3/library/datetime.html#module-datetime) de Python.

On peut par exemple ajouter des intervalles de temps, ou bien calculer des √©carts √† une date de r√©f√©rence.

::: {.callout-note}
Par d√©faut, les diff√©rences de temps sont indiqu√©es dans une unit√© temporelle.
Si l'on veut transformer une dur√©e en un nombre, il faut utiliser les m√©thodes `Series.dt.total_...`.

:::

```{python}
df.with_columns(
    date = pl.col("date") + pl.duration(days=1),
    date_diff = (pl.col('date') - pl.date(2022, 1, 1)).dt.total_days()
)
```

### Joindre des tables

Dans le cadre d'une analyse de donn√©es, il est courant de vouloir combiner diff√©rentes sources de donn√©es.
Cette combinaison peut se faire verticalement (un DataFrame par dessus l'autre), par exemple lorsque l'on souhaite combiner deux mill√©simes d'une m√™me enqu√™te afin de les analyser conjointement.
La combinaison peut √©galement se faire horizontalement (c√¥te √† c√¥te) selon une ou plusieurs cl√©(s) de jointure, souvent dans le but d'enrichir une source de donn√©es √† partir d'une autre source portant sur les m√™mes unit√©s statistiques.

#### Concat√©ner des tables

La concat√©nation verticale de tables se fait √† l'aide de la fonction [`concat()`](https://docs.pola.rs/api/python/stable/reference/api/polars.concat.html#polars.concat) de Polars.

```{python}

df1 = pl.DataFrame(
    data = {
        "var1": [1, 5],
        "var2": [3, 7],
        "date": ["2022-01-01", "2022-01-02"],
        "sample": ["sample1", "sample1"]
    }
)

df2 = pl.DataFrame(
    data = {
        "var1": [9, 13],
        "var2": [11, 15],
        "date": ["2023-01-01", "2023-01-02"],
        "sample": ["sample2", "sample2"]
    }
)

pl.concat([df1, df2])

```

A la grande diff√©rence de `Pandas`, **l'ordre des variables dans les deux DataFrames importe**.

Ainsi, le m√™me code avec `df2` d√©fini comme ci-apr√®s va renvoyer une erreur.
```{.python}

df2 = pl.DataFrame(
    data = {
        "var1": [9, 13],
        "date": ["2023-01-01", "2023-01-02"],
        "var2": [11, 15],
        "sample": ["sample2", "sample2"]
    }
)

```

Pour contourner ce probl√®me, il suffit de pr√©f√©rer une m√©thode de type `join` avec `df1.join(df2, on=df1.columns, how="full", coalesce=True)` (*cf.* ci-apr√®s).


#### Fusionner des tables

La fusion de tables est une op√©ration qui permet d'associer des lignes de deux DataFrames diff√©rents en se basant sur une ou plusieurs cl√©s communes, similaire aux jointures dans les bases de donn√©es SQL.
Diff√©rents types de jointure sont possible selon les donn√©es que l'on souhaite conserver, dont les principaux sont repr√©sent√©s sur le graphique suivant.

![](img/joins.png)

Source : [lien](https://medium.com/swlh/merging-dataframes-with-pandas-pd-merge-7764c7e2d46d)

En Polars, les jointures se font avec la fonction [`join()`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.join.html#polars.DataFrame.join).
Pour r√©aliser une jointure, on doit sp√©cifier (au minimum) deux informations :

- le type de jointure : par d√©faut, Polars effectue une jointure de type `inner`.
Le param√®tre `how` permet de sp√©cifier d'autres types de jointure ;

- la cl√© de jointure.
On sp√©cifie souvent une colonne pr√©sente dans le DataFrame comme cl√© de jointure (param√®tre `on` si la colonne porte le m√™me nom dans les deux DataFrame, ou `left_on` et `right_on` sinon).


Par d√©faut, les colonnes pr√©sentes dans les deux colonnes seront conserv√©es avec des suffixes '_right' ou '_left'.
Pour √©viter ce comportement, il faut pr√©ciser l'argument `coalesce`.
On peut aussi sp√©cifier l'ordre dans lequel les colonnes apparaissent avec l'argument `maintain_order`.

::: {.callout-note}
On peut aussi tout √† fait joindre selon des crit√®res diff√©rents que l'√©galit√© : par exemple, joint cette ligne avec toute ligne dont la date est ant√©rieure.
Les m√©thodes utilis√©es sont [`join_asof()`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.join_asof.html) et [`join_where()`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.join_where.html)`
:::


Analysons la diff√©rence entre les diff√©rents types de jointure √† travers des exemples.

```{python}
df_a = pl.DataFrame({
    'key': ['K0', 'K1', 'K2', 'K3', 'K4'],
    'A': ['A0', 'A1', 'A2', 'A3', 'A4'],
    'B': ['B0', 'B1', 'B2', 'B3', 'A4']
})

df_b = pl.DataFrame({
    'key': ['K0', 'K1', 'K2', 'K5', 'K6'],
    'C': ['C0', 'C1', 'C2', 'C5', 'C6'],
    'D': ['D0', 'D1', 'D2', 'D5', 'D6']
})

print(df_a)
print(df_b)
```

- **Inner Join**
La jointure de type `inner` conserve les observations dont la cl√© est pr√©sente dans les deux DataFrame.

```{python}
df_a.join(df_b, on='key')
```

::: {.callout-warning title="Jointures inner"}
La jointure de type `inner` est la plus intuitive : elle ne cr√©e g√©n√©ralement pas de valeurs manquantes et permet donc de travailler directement sur la table fusionn√©e.
Mais attention : si beaucoup de cl√©s ne sont pas pr√©sentes dans les deux DataFrames √† la fois, une jointure `inner` peut aboutit √† des pertes importantes de donn√©es, et donc √† des r√©sultats finaux biais√©s.
Dans ce cas, il vaut mieux choisir une jointure √† gauche ou √† droite, selon la source que l'on cherche √† enrichir et pour laquelle il est donc le plus important de limiter les pertes de donn√©es.
:::

- **Left join**
Une jointure de type `left` conserve toutes les observations contenues dans le DataFrame de gauche (le DataFrame √† qui on applique la m√©thode `.join()`).
Par cons√©quent, si des cl√©s sont pr√©sentes dans le DataFrame de gauche mais pas dans celui de droite, le DataFrame final contient des valeurs manquantes au niveau de ces observations (pour les variables du DataFrame de droite).

```{python}
df_a.join(df_b, how="left", on='key')
```

- **Full join**
La jointure de type `full` contient toutes les observations et variables contenues dans les deux DataFrame.
Ainsi, l'information retenue est maximale, mais en contrepartie les valeurs manquantes peuvent √™tre assez nombreuses.
Il sera donc n√©cessaire de bien traiter les valeurs manquantes avant de proc√©der aux analyses.

```{python}
df_a.join(df_b, how="full", on='key')
```

```{python}
df_a.join(df_b, how="full", on='key', coalesce=True)
```

## Exercices

### Questions de compr√©hension


- 1/ Qu'est-ce qu'un DataFrame dans le contexte de Polars et √† quel type de structure de donn√©es peut-on le comparer dans le langage Python ?

- 2/ Quels sont les avantages de Polars ?

- 3/ Quel est le lien entre Series et DataFrame dans Polars ?

- 4/ Comment sont structur√©es les donn√©es dans un DataFrame Polars ?

- 6/ Quelles m√©thodes pouvez-vous utiliser pour explorer un DataFrame inconnu et en apprendre davantage sur son contenu et sa structure ?

- 8/ Comment s'applique le principe d'√©valuation paresseuse dans Polars et pourquoi est-ce avantageux pour manipuler les donn√©es ?

- 9/ Comment Polars repr√©sente-t-il les valeurs manquantes et quel impact cela a-t-il sur les calculs et les transformations de donn√©es ?

- 10/ Quelle est la diff√©rence entre concat√©ner deux DataFrames et les joindre via une jointure, et quand utiliseriez-vous l'une plut√¥t que l'autre ?


::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

- 1/ Un DataFrame dans Pandas est une structure de donn√©es bidimensionnelle, comparable √† un tableau ou une feuille de calcul Excel. Dans le contexte Python, on peut le comparer √† un dictionnaire d'arrays NumPy, o√π les cl√©s sont les noms des colonnes et les valeurs sont les colonnes elles-m√™mes.

- 2/ La diff√©rence principale entre un array NumPy et une Series Pandas est que la Series peut contenir des donn√©es √©tiquet√©es, c'est-√†-dire qu'elle a un index qui lui est associ√©, permettant des acc√®s et des manipulations par label.

- 3/ Un DataFrame est essentiellement une collection de Series. Chaque colonne d'un DataFrame est une Series, et toutes ces Series partagent le m√™me index, qui correspond aux √©tiquettes des lignes du DataFrame.

- 4/ Les donn√©es dans un DataFrame Pandas sont structur√©es en colonnes et en lignes. Chaque colonne peut contenir un type de donn√©es diff√©rent (num√©rique, cha√Æne de caract√®res, bool√©en, etc.), et chaque ligne repr√©sente une observation.

- 5/ L'index dans un DataFrame Pandas sert √† identifier de mani√®re unique chaque ligne du DataFrame. Il permet d'acc√©der rapidement aux lignes, de r√©aliser des jointures, de trier les donn√©es et de faciliter les op√©rations de regroupement.

- 6/ Pour explorer un DataFrame inconnu, on peut utiliser df.head() pour voir les premi√®res lignes, df.tail() pour les derni√®res, df.info() pour obtenir un r√©sum√© des types de donn√©es et des valeurs manquantes, et df.describe() pour des statistiques descriptives.

- 7/ Assigner le r√©sultat d'une op√©ration √† une nouvelle variable cr√©e une copie du DataFrame avec les modifications appliqu√©es. Utiliser une m√©thode avec inplace=True modifie le DataFrame original sans cr√©er de copie, ce qui peut √™tre plus efficace en termes de m√©moire.

- 8/ Pandas repr√©sente les valeurs manquantes avec l'objet `nan` (Not a Number) de `Numpy` pour les donn√©es num√©riques et avec None ou pd.NaT pour les dates/temps. Ces valeurs manquantes sont g√©n√©ralement ignor√©es dans les calculs de fonctions statistiques, ce qui peut affecter les r√©sultats si elles ne sont pas trait√©es correctement.

- 9/ Concat√©ner consiste √† assembler des DataFrames en les empilant verticalement ou en les alignant horizontalement, principalement utilis√© lorsque les DataFrames ont le m√™me sch√©ma ou lorsque vous souhaitez empiler les donn√©es. Les jointures, inspir√©es des op√©rations JOIN en SQL, combinent les DataFrames sur la base de valeurs de cl√©s communes et sont utilis√©es pour enrichir un ensemble de donn√©es avec des informations d'un autre ensemble.

</details>

:::

### Plusieurs mani√®res de cr√©er un DataFrame

Dans la cellule suivante, nous avons r√©cup√©r√© des donn√©es de caisses sur les ventes de diff√©rentes enseignes. Les donn√©es sont cependant pr√©sent√©es de deux mani√®res diff√©rentes, dans un cas sous forme d'observations (chaque liste contient les donn√©es d'une ligne), dans l'autre sous forme de variables (chaque liste contient les donn√©es d'une colonne).

```{python}
data_list1 = [
    ['Carrefour', '01.1.1', 3, 1.50],
    ['Casino', '02.1.1', 2, 2.30],
    ['Lidl', '01.1.1', 7, 0.99],
    ['Carrefour', '03.1.1', 5, 5.00],
    ['Casino', '01.1.1', 10, 1.20],
    ['Lidl', '02.1.1', 1, 3.10]
]

data_list2 = [
    ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    [3, 2, 7, 5, 10, 1],
    [1.50, 2.30, 0.99, 5.00, 1.20, 3.10]
]
```

L'objectif est de construire dans les deux cas un m√™me DataFrame qui contient chacune des 6 observations et des 4 variables, avec les m√™mes noms dans les deux DataFrame. A chaque cas va correspondre une structure de donn√©es plus adapt√©e en entr√©e, dictionnaire ou liste de listes... fa√Ætes le bon choix ! On v√©rifiera que les deux DataFrames sont identiques √† l'aide de la m√©thode [equals()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.equals.html).

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
data_list1 = [
    ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    [3, 2, 7, 5, 10, 1],
    [1.50, 2.30, 0.99, 5.00, 1.20, 3.10]
]

data_list2 = [
    ['Carrefour', '01.1.1', 3, 1.50],
    ['Casino', '02.1.1', 2, 2.30],
    ['Lidl', '01.1.1', 7, 0.99],
    ['Carrefour', '03.1.1', 5, 5.00],
    ['Casino', '01.1.1', 10, 1.20],
    ['Lidl', '02.1.1', 1, 3.10]
]

# Si les donn√©es sont sous forme de colonnes : √† partir d'un dictionnaire
data_dict = {
    'enseigne': data_list1[0],
    'produit': data_list1[1],
    'quantite': data_list1[2],
    'prix': data_list1[3]
}

df_from_dict = pd.DataFrame(data_dict)

# Si les donn√©es sont sous forme de lignes : √† partir d'une liste de listes
columns = ['enseigne', 'produit', 'quantite', 'prix']
df_from_list = pd.DataFrame(data_list2, columns=columns)

# V√©rification
df_from_dict.equals(df_from_list)
```

</details>

:::

### S√©lection de donn√©es dans un DataFrame

Un DataFrame Pandas est cr√©√© avec des donn√©es de caisse (m√™mes donn√©es que l'exercice pr√©c√©dent).

```{python}
data = {
    'enseigne': ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    'produit': ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    'quantite': [3, 2, 7, 5, 10, 1],
    'prix': [1.50, 2.30, 0.99, 5.00, 1.20, 3.10],
    'date_heure': pd.to_datetime(["2022-01-01 14:05", "2022-01-02 09:30",
                                  "2022-01-03 17:45", "2022-01-04 08:20",
                                  "2022-01-05 19:00", "2022-01-06 16:30"])
}

df = pd.DataFrame(data)
```

Utilisez les m√©thodes `loc` et `iloc` pour s√©lectionner des donn√©es sp√©cifiques :

- S√©lectionner les donn√©es de la premi√®re ligne.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
print(df.iloc[0])
```

</details>

:::

- S√©lectionner toutes les donn√©es de la colonne "prix".

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
print(df.loc[:, 'prix'])
```

</details>

:::

- S√©lectionner les lignes correspondant √† l'enseigne "Carrefour" uniquement.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
print(df.loc[df['enseigne'] == 'Carrefour'])
```

</details>

:::

- S√©lectionner les quantit√©s achet√©es pour les produits classifi√©s "01.1.1" (Pain).

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
print(df.loc[df['produit'] == '01.1.1', 'quantite'])
```

</details>

:::

- S√©lectionner les donn√©es des colonnes "enseigne" et "prix" pour toutes les lignes.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
print(df.loc[:, ['enseigne', 'prix']])
```

</details>

:::

- S√©lectionner les lignes o√π la quantit√© achet√©e est sup√©rieure √† 5.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
print(df.loc[df['quantite'] > 5])
```

</details>

:::

- Filtrer pour s√©lectionner toutes les transactions qui ont eu lieu apr√®s 15h.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
print(df.loc[df['date_heure'].dt.hour > 15])
```

</details>

:::

- S√©lectionner les transactions qui ont eu lieu le "2022-01-03".

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
print(df.loc[df['date_heure'].dt.date == pd.to_datetime('2022-01-03').date()])
```

</details>

:::

### Exploration du fichier des pr√©noms

Le fichier des pr√©noms contient des donn√©es sur les pr√©noms attribu√©s aux enfants n√©s en France entre 1900 et 2021. Ces donn√©es sont disponibles au niveau France, par d√©partement et par r√©gion, √† l'adresse suivante : [https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262](https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262). L'objectif de ce tutoriel est de proposer une analyse de ce fichier, du nettoyage des donn√©es au statistiques sur les pr√©noms.

#### Partie 1 : Import et exploration des donn√©es


- Importez les donn√©es dans un DataFrame en utilisant cette [URL](https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip).
- Visualisez un √©chantillon des donn√©es. Rep√©rez-vous d'√©ventuelles anomalies ?
- Affichez les principales informations du DataFrame. Rep√©rez d'√©ventuelles variables dont le type serait incorrect, ou bien d'√©ventuelles valeurs manquantes.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
url = "https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip"
df_prenoms = pd.read_csv(url, sep=";")

df_prenoms.head(10)
df_prenoms.sample(n=50)

df_prenoms.info()
```

</details>

:::

#### Partie 2 : Nettoyage des donn√©es


- L'output de la m√©thode `info()` sugg√®re des valeurs manquantes dans la colonne des pr√©noms. Affichez ces lignes. V√©rifiez que ces valeurs manquantes sont correctement sp√©cifi√©es.
- L'output de m√©thode `head()` montre une modalit√© r√©currente "_PRENOMS_RARES" dans la colonne des pr√©noms. Quelle proportion des individus de la base cela concerne-t-il ? Convertir ces valeurs en `np.nan`.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
print(df_prenoms[df_prenoms["preusuel"].isna()])
prop_rares = df_prenoms.groupby("preusuel")["nombre"].sum()["_PRENOMS_RARES"] / df_prenoms["nombre"].sum()
print(prop_rares)  # ~ 2 % de la base
df_prenoms = df_prenoms.replace('_PRENOMS_RARES', None)
```

</details>

:::

- On remarque que les pr√©noms de personnes dont l'ann√©e de naissance n'est pas connue sont regroup√©s sous la modalit√© `XXXX`. Quelle proportion des individus de la base cela concerne-t-il ? Convertir ces valeurs en `np.nan`.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
prop_xxxx = df_prenoms.groupby("annais")["nombre"].sum()["XXXX"] / df_prenoms["nombre"].sum()
print(prop_xxxx)  # ~ 1 % de la base
df_prenoms = df_prenoms.replace('XXXX', None)
```

</details>

:::

- Supprimer les lignes contenant des valeurs manquantes de l'√©chantillon.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_prenoms = df_prenoms.dropna()
```

</details>

:::

- Convertissez la colonne `annais` en type num√©rique et la colonne `sexe` en type cat√©goriel.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_prenoms['annais'] = pd.to_numeric(df_prenoms['annais'])
df_prenoms['sexe'] = df_prenoms['sexe'].astype('category')
```

</details>

:::

- V√©rifiez avec la m√©thode `info()` que le nettoyage a √©t√© correctement appliqu√©.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_prenoms.info()
```

</details>

:::

#### Partie 3 : Statistiques descriptives sur les naissances


- La [documentation](https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262#documentation) du fichier nous informe qu'on peut consid√©rer les donn√©es comme quasi-exhaustives √† partir de 1946. Pour cette partie seulement, filtrer les donn√©es pour ne conserver que les donn√©es ult√©rieures.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_prenoms_post_1946 = df_prenoms[df_prenoms["annais"] >= 1946]
```

</details>

:::

- Calculez le nombre total de naissances par sexe.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
births_per_sex = df_prenoms_post_1946.groupby('sexe')['nombre'].sum()
print(births_per_sex)
```

</details>

:::

- Identifiez les cinq ann√©es ayant le plus grand nombre de naissances.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
top5_years = df_prenoms_post_1946.groupby('annais')['nombre'].sum().nlargest(5)
print(top5_years)
```

</details>

:::

#### Partie 4 : Analyse des pr√©noms


- Identifiez le nombre total de pr√©noms uniques dans le DataFrame.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
total_unique_names = df_prenoms['preusuel'].nunique()
print(total_unique_names)
```

</details>

:::

- Compter le nombre de personnes poss√©dant un pr√©nom d'une seule lettre.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
single_letter_names = df_prenoms[df_prenoms['preusuel'].str.len() == 1]['nombre'].sum()
print(single_letter_names)
```

</details>

:::

- Cr√©ez une "fonction de popularit√©" qui, pour un pr√©nom donn√©, affiche l'ann√©e o√π il a √©t√© le plus donn√© ainsi que le nombre de fois o√π il a √©t√© donn√© cette ann√©e-l√†.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
def popularite_par_annee(df, prenom):
    # Filtrer le DataFrame pour ne garder que les lignes correspondant au pr√©nom donn√©
    df_prenom = df[df['preusuel'] == prenom]

    # Grouper par ann√©e, sommer les naissances et identifier l'ann√©e avec le maximum de naissances
    df_agg = df_prenom.groupby('annais')['nombre'].sum()
    annee_max = df_agg.idxmax()
    n_max = df_agg[annee_max]

    print(f"Le pr√©nom '{prenom}' a √©t√© le plus donn√© en {annee_max}, avec {n_max} naissances.")

# Test de la fonction avec un exemple
popularite_par_annee(df_prenoms, 'ALFRED')
```

</details>

:::

- Cr√©ez une fonction qui, pour un sexe donn√©, renvoie un DataFrame contenant le pr√©nom le plus donn√© pour chaque d√©cennie.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
def popularite_par_decennie(df, sexe):
    # Filtrage sur le sexe
    df_sub = df[df["sexe"] == sexe]

    # Calcul de la variable d√©cennie
    df_sub["decennie"] = (df_sub["annais"] // 10) * 10

    # Calculer la somme des naissances pour chaque pr√©nom et chaque d√©cennie
    df_counts_decennie = df_sub.groupby(["preusuel", "decennie"])["nombre"].sum().reset_index()

    # Trouver l'indice du pr√©nom le plus fr√©quent pour chaque d√©cennie
    idx = df_counts_decennie.groupby("decennie")["nombre"].idxmax()

    # Utiliser l'indice pour obtenir les lignes correspondantes du DataFrame df_counts_decennie
    df_popularite_decennie = df_counts_decennie.loc[idx].set_index("decennie")

    return df_popularite_decennie

# Test de la fonction avec un exemple
popularite_par_decennie(df_prenoms, sexe=2)
```

</details>

:::

### Calcul d'une empreinte carbone par habitant au niveau communal

L'objectif de cet exercice est de calculer une empreinte carbone par habitant au niveau communal. Pour cela, il va falloir combiner deux sources de donn√©es :

- les populations l√©gales au niveau des communes, issues du recensement de la population ([source](https://www.insee.fr/fr/statistiques/6683037))

- les √©missions de gaz √† effet de serre estim√©es au niveau communal par l‚ÄôADEME ([source](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_))

Cet exercice constitue une version simplifi√©e d'un [TP complet pour la pratique de Pandas](https://pythonds.linogaliana.fr/content/manipulation/02b_pandas_TP.html#importer-les-donn%C3%A9es) propos√© par Lino Galiana dans son [cours √† l'ENSAE](https://pythonds.linogaliana.fr/).

#### Partie 1 : Exploration des donn√©es sur les populations l√©gales communales

- Importez le fichier CSV `communes.csv`.
- Utilisez les m√©thodes `.sample()`, `.info()` et `.describe()` pour obtenir un aper√ßu des donn√©es.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_pop_communes = pd.read_csv("data/communes.csv", sep=";")

df_pop_communes.sample(10)
df_pop_communes.info()
df_pop_communes.describe()
```

</details>

:::

- Identifiez et retirez les lignes correspondant aux communes sans population.
- Supprimez les colonnes "PMUN" et "PCAP", non pertinentes pour l'analyse.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
n_communes_0_pop = df_pop_communes[df_pop_communes["PTOT"] == 0].shape[0]
print(n_communes_0_pop)
df_pop_communes = df_pop_communes[df_pop_communes["PTOT"] > 0]

df_pop_communes = df_pop_communes.drop(columns=["PMUN", "PCAP"])
```

</details>

:::

Les communes qui ont les noms les plus longs sont-elles aussi les communes les moins peupl√©es ? Pour le savoir :
- Cr√©ez une nouvelle variable qui contient le nombre de caract√®res de chaque commune √† l'aide de la m√©thode [str.len()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.len.html)
- Calculez la corr√©lation entre cette variable et la population totale avec la m√©thode [corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_pop_communes_stats = df_pop_communes.copy()
df_pop_communes_stats['longueur'] = df_pop_communes_stats['COM'].str.len()
df_pop_communes_stats['longueur'].corr(df_pop_communes_stats['PTOT'])
```

</details>

:::

#### Partie 2 : Exploration des donn√©es sur les √©missions communales

- Importez les donn√©es d'√©mission depuis cette [URL](https://data.ademe.fr/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/data-files/IGT%20-%20Pouvoir%20de%20r%C3%A9chauffement%20global.csv)
- Utilisez les m√©thodes `.sample()`, `.info()` et `.describe()` pour obtenir un aper√ßu des donn√©es.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
url_ademe = "https://data.ademe.fr/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/data-files/IGT%20-%20Pouvoir%20de%20r%C3%A9chauffement%20global.csv"
df_emissions = pd.read_csv(url_ademe)

df_emissions.sample(10)
df_emissions.info()
df_emissions.describe()
```

</details>

:::

- Y a-t-il des lignes avec des valeurs manquantes pour toutes les colonnes d'√©mission ? V√©rifiez-le √† l'aide des m√©thodes [isnull()](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) et [all()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.all.html).

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_emissions_num = df_emissions.select_dtypes(['number'])
only_nan = df_emissions_num[df_emissions_num.isnull().all(axis=1)]
only_nan.shape[0]
```

</details>

:::

- Cr√©ez une nouvelle colonne qui donne les √©missions totales par commune
- Afficher les 10 communes les plus √©mettrices. Qu'observez-vous dans les r√©sultats ?

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_emissions['emissions_totales'] = df_emissions.sum(axis = 1, numeric_only = True)

df_emissions.sort_values(by="emissions_totales", ascending=False).head(10)
```

</details>

:::

- Il semble que les postes majeurs d'√©missions soient "Industrie hors-√©nergie" et "Autres transports international". Pour v√©rifier si cette conjecture tient, calculer la corr√©lation entre les √©missions totales et les postes sectoriels d'√©missions √† l'aide de la m√©thode [corrwith()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corrwith.html).

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_emissions.corrwith(df_emissions["emissions_totales"], numeric_only=True)
```

</details>

:::

- Extraire du code commune le num√©ro de d√©partement dans une nouvelle variable
- Calculer les √©missions totales par d√©partement
- Afficher les 10 principaux d√©partements √©metteurs. Les r√©sultats sont-ils logiques par rapport √† l'analyse au niveau communal ?

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_emissions["dep"] = df_emissions["INSEE commune"].str[:2]
df_emissions.groupby("dep").agg({"emissions_totales": "sum"}).sort_values(by="emissions_totales", ascending=False).head(10)
```

</details>

:::

#### Partie 3 : V√©rifications pr√©alables pour la jointure des sources de donn√©es

Pour effectuer une jointure, il est toujours pr√©f√©rable d'avoir une cl√© de jointure, i.e. une colonne commune aux deux sources, qui identifie uniquement les unit√©s statistiques. L'objet de cette partie est de trouver la cl√© de jointure pertinente.

- V√©rifiez si la variable contenant les noms de commune contient des doublons

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
doublons = df_pop_communes.groupby('COM').count()['DEPCOM']
doublons = doublons[doublons>1]
doublons = doublons.reset_index()
doublons
```

</details>

:::

- Filtrez dans le DataFrame initial les communes dont le nom est dupliqu√©, et triez-le par code commune. Les doublons semblent-ils probl√©matiques ?

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_pop_communes_doublons = df_pop_communes[df_pop_communes["COM"].isin(doublons["COM"])]
df_pop_communes_doublons.sort_values('COM')
```

</details>

:::

- V√©rifiez que les codes commune identifient de mani√®re unique la commune associ√©e

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
(df_pop_communes_doublons.groupby("DEPCOM")["COM"].nunique() != 1).sum()
```

</details>

:::

- Affichez les communes pr√©sentes dans les donn√©es communales mais pas dans les donn√©es d'√©missions, et inversement. Qu'en concluez-vous ?

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
## Observations qui sont dans les pop l√©gales mais pas dans les donn√©es d'√©missions
df_pop_communes[~df_pop_communes["DEPCOM"].isin(df_emissions["INSEE commune"])]

## Observations qui sont dans les donn√©es d'√©missions mais pas dans les pop l√©gales
df_emissions[~df_emissions["INSEE commune"].isin(df_pop_communes["DEPCOM"])]
```

</details>

:::

#### Partie 4 : Calcul d'une empreinte carbone par habitant pour chaque commune

- Joindre les deux DataFrames √† l'aide de la fonction √† partir du code commune. Attention : les variables ne s'appellent pas de la m√™me mani√®re des deux c√¥t√©s !

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_emissions_pop = pd.merge(df_pop_communes, df_emissions, how="inner", left_on="DEPCOM", right_on="INSEE commune")
df_emissions_pop
```

</details>

:::

- Calculer une empreinte carbone pour chaque commune, correspondant aux √©missions totales de la commune divis√©es par sa population totale.
- Affichez les 10 communes avec les empreintes carbones les plus √©lev√©es.
- Les r√©sultats sont-ils identiques √† ceux avec les √©missions totales ? Qu'en concluez-vous ?

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_emissions_pop["empreinte_carbone"] = df_emissions_pop["emissions_totales"] / df_emissions_pop["PTOT"]
df_emissions_pop.sort_values("empreinte_carbone", ascending=False).head(10)
```

</details>

:::

### Analyse de l'√©volution d'un indice de production

Vous avez √† disposition dans le dossier `data/` deux jeux de donn√©es CSV :

- `serie_glaces_valeurs.csv` contient les valeurs mensuelles de l'indice de prix de production de l'industrie fran√ßaise des glaces et sorbets
- `serie_glaces_metadonnees.csv` contient les m√©tadonn√©es associ√©es, notamment les codes indiquant le statut des donn√©es.

L'objectif est d'utiliser `Pandas` pour calculer :

- l'√©volution de l'indice entre chaque p√©riode (mois)
- l'√©volution de l'indice en glissement annuel (entre un mois donn√© et le m√™me mois l'ann√©e suivante).

#### Partie 1 : Import des donn√©es

- Importez les deux fichiers CSV dans des DataFrames. Attention, dans les deux cas, il y a des lignes superflues avant les donn√©es, qu'il faudra sauter √† l'aide du param√®tre `skiprows` de la fonction [read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).
- Donnez des noms simples et pertinents aux diff√©rentes variables.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_valeurs = pd.read_csv('data/serie_glaces_valeurs.csv', delimiter=';',
                         skiprows=4, names=["periode", "indice", "code"])
df_metadata = pd.read_csv('data/serie_glaces_metadonnees.csv', delimiter=';',
                          skiprows=5, names=["code", "signification"])
```

</details>

:::

#### Partie 2 : Filtrage des donn√©es pertinentes

- Fusionner les deux DataFrames afin de r√©cup√©rer la signification des codes pr√©sents dans les donn√©es.
- Filtrer les donn√©es de sorte √† ne conserver que les donn√©es de type "Valeur normale".
- Supprimer les colonnes li√©es aux codes, dont nous n'avons plus besoin pour la suite.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_merged = pd.merge(df_valeurs, df_metadata, how='left', on='code')

df_clean = df_merged[df_merged['code'] == "A"]
df_clean = df_clean[["periode", "indice"]]
```

</details>

:::

#### Partie 3 : Pr√©-traitement des donn√©es

V√©rifiez si les types des variables sont pertinents selon leur nature. Sinon, convertissez-les avec les fonctions idoines.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_clean.info()
df_clean['periode'] = pd.to_datetime(df_clean['periode'])
df_clean['indice'] = pd.to_numeric(df_clean['indice'])
df_clean.info()
```

</details>

:::

#### Partie 4 : Calcul de l'√©volution p√©riodique

- Utilisez la m√©thode [shift()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html) pour cr√©er une nouvelle colonne qui contiendra l'indice du trimestre pr√©c√©dent
- Calculez la diff√©rence entre l'indice actuel et l'indice d√©cal√© pour obtenir l'√©volution (en pourcentage) d'un trimestre √† l'autre

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_clean['indice_prec'] = df_clean['indice'].shift(1)
df_clean['evo'] = ((df_clean['indice'] - df_clean['indice_prec']) / df_clean['indice_prec']) * 100

## M√©thode alternative
df_clean['evo_alt'] = df_clean['indice'].pct_change(periods=1) * 100
```

</details>

:::

#### Partie 5 : Calcul de l'√©volution glissante sur 12 mois

Comme vous avez pu le voir dans la solution de l'exercice pr√©c√©dent, la m√©thode [pct_change()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html) permet pr√©cis√©ment de calculer une √©volution entre deux p√©riodes. Utiliser cette m√©thode pour calculer une √©volution (en pourcentage) en glissement annuel pour chaque mois.

```{python}
# Testez votre r√©ponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```{python}
df_clean["evo_glissement_annuel"] = df_clean['indice'].pct_change(periods=12) * 100
df_clean.head(20)
```

</details>

:::

:::













::: {.content-visible when-profile="en"}

# Working with tabular data using Pandas

Statistical analysis is generally based on **tabular data**, where each row represents an observation and each column a variable. To handle this type of data and easily apply standard data analysis methods, dedicated objects have been designed: `DataFrames`. Users of `R` are well acquainted with this data structure, which is native to this statistics-oriented language. In `Python`, a general-purpose language, this object does not exist natively. Fortunately, a very comprehensive and convenient library, designed as an overlay to `NumPy`, introduces the `DataFrame` object in `Python` and allows for simple and intuitive data manipulation and analysis: `Pandas`.

::: {.callout-note}
Pandas is the central element of the data science ecosystem in Python, offering virtually infinite data processing capabilities. Moreover, there are generally multiple ways to perform the same operation in Pandas. Consequently, this chapter is particularly long and dense with new features. The goal is not to memorize all the methods presented throughout this chapter, but rather to have a general overview of what is possible in order to use the right tools in projects. In particular, the end-of-chapter exercises and the mini-projects at the end of the course will provide an opportunity to apply this new knowledge to concrete problems.
:::

Let's start by importing the `Pandas` library. The common usage is to give it the alias `pd` to simplify future calls to the package's objects and functions. We also import `NumPy` as we will compare the fundamental objects of the two packages.

```{python}
import pandas as pd
import numpy as np
```

## Data structures

To fully understand how `Pandas` works, it is important to focus on its fundamental objects. We will therefore first study the `Series`, whose concatenation allows us to build a `DataFrame`.

### The `Series`

A Series is a one-dimensional data container that can hold any type of data (integers, strings, Python objects...). However, a Series is of a given type: a Series containing only integers will be of type `int`, and a Series containing objects of different natures will be of type `object`. Let's build our first Series from a list to check this behavior.

```{python}
l = [1, "X", 3]
s = pd.Series(l)
print(s)
```

In particular, we can access the data of a Series by position, as for a list or an array.

```{python}
print(s[1])
```

At first glance, we do not see much difference between a Series and a one-dimensional `NumPy` array. However, there is a significant difference: the presence of an index. The observations have an associated label. When we create a Series without specifying anything, the index is automatically set to the integers from 0 to n-1 (with n being the number of elements in the Series). But it is possible to pass a specific index (e.g., dates, town names, etc.).

```{python}
s = pd.Series(l, index=["a", "b", "c"])
print(s)
```

This allows us to access the data by label:

```{python}
s["b"]
```

This difference may seem minor at first, but it becomes essential for constructing the DataFrame. For the rest, Series behave very similarly to NumPy arrays: calculations are vectorized, we can directly sum two Series, etc. Moreover, we can easily convert a Series into an array via the `values` attribute. This naturally loses the index...

```{python}
s = pd.Series(l, index=["a", "b", "c"])
s.values
```

### The `DataFrame`

Fundamentally, a DataFrame consists of a collection of Series, aligned by their indexes. This concatenation thus constructs a data table, with Series corresponding to columns, and the index identifying the rows. The following figure ([source](https://www.geeksforgeeks.org/creating-a-pandas-dataframe/)) helps to understand this data structure.

![](img/structure_df.png){fig-align="center" width="800"}

A DataFrame can be constructed in multiple ways. In practice, we generally build a DataFrame directly from tabular data files (e.g., CSV, Excel), rarely by hand. So, we will only illustrate the most common manual construction method: from a data dictionary.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, None, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df
```

A Pandas DataFrame has a set of useful attributes that we will discover throughout this tutorial. For now, let's focus on the most basic ones: the index and the column names. By default, the index is initialized, as for Series, to the list of positions of the observations. We could have specified an alternative index when constructing the DataFrame by specifying the `index` argument of the `pd.DataFrame` function.

```{python}
df.index
```

```{python}
df.columns
```

Often, rather than specifying an index manually during the construction of the DataFrame, we will want to use a certain column of the DataFrame as an index. We use the `set_index` method associated with DataFrames for this.

```{python}
df = df.set_index("date")
df
```

The index attribute has naturally changed:

```{python}
df.index
```

## Selecting data

When manipulating tabular data, it is common to want to extract specific columns from a `DataFrame`. This extraction is simple with `Pandas` using square brackets.

### Selecting columns

#### Selecting a single column

To extract a single column, we can use the following syntax:

```{python}
selected_column = df["var1"]
selected_column
```

The `selected_column` object here returns the column named `var1` from the `DataFrame` `df`. But what type is this object? To answer this question, we use the `type()` function:

```{python}
type(selected_column)
```

As we can see, the result is a `Series`, which is a one-dimensional object in `Pandas`.

Another useful attribute to know is `shape`. It allows us to know the dimension of the object. For a `Series`, `shape` will return a tuple whose first element indicates the number of rows.

```{python}
selected_column.shape
```

#### Selecting multiple columns

To extract multiple columns, just pass a list of the desired column names:

```{python}
selected_columns = df[["var1", "var2", "experiment"]]
selected_columns
```

This snippet shows the columns `var1`, `var2`, and `experiment` from the `DataFrame` `df`. Let's now check its type:

```{python}
type(selected_columns)
```

The result is a `DataFrame` because it is a two-dimensional object. We can also check its shape with the `shape` attribute. In this case, the tuple returned by `shape` will contain two elements: the number of rows and the number of columns.

```{python}
selected_columns.shape
```

### Selecting rows

#### Using `loc` and `iloc`

When we want to select specific rows in a DataFrame, we can use two main methods: `loc` and `iloc`.

- `iloc` allows selecting rows and columns by their position, i.e., by numeric indices.

Example, selecting the first 3 rows:

```{python}
df.iloc[0:3, :]
```

- `loc` works with labels. If the DataFrame's indexes are numbers, they resemble positions, but this is not necessarily the case. It is crucial to note that, unlike `iloc`, with `loc`, the end index is included in the selection.

```{python}
df.loc["2022-01-01":"2022-01-03", :]
```

#### Filtering data based on conditions

In practice, rather than selecting rows based on positions or labels, we often want to filter a DataFrame based on certain conditions. In this case, we primarily use boolean filters.

- **Inequalities**: We might want to keep only the rows that meet a certain condition.

Example, filtering rows where the value of the `var2` column is greater than 0:

```{python}
df[df['var2'] >= 0]
```

- **Membership with `isin`**: If we want to filter data based on a list of possible values, the `isin` method is very useful.

Example, to keep only the rows where the `experiment` column has values 'test' or 'validation':

```{python}
df[df['experiment'].isin(['train', 'validation'])]
```

These methods can be combined to create more complex conditions. It is also possible to use logical operators (`&` for "and", `|` for "or") to combine multiple conditions. Be careful to enclose each condition in parentheses when combining them.

Example, selecting rows where `var2` is greater than 0 and `experiment` is equal to 'test' or 'validation':

```{python}
df[(df['var2'] >= 0) & (df['experiment'].isin(['train', 'validation']))]
```

## Exploring tabular data

In public statistics, the starting point is generally not the manual generation of data but rather pre-existing tab

ular files. These files, whether from surveys, administrative databases, or other sources, constitute the raw material for any subsequent analysis. Pandas offers powerful tools to import these tabular files and explore them for further manipulations.

### Importing and exporting data

#### Importing a CSV file

As we saw in a previous lab, the CSV format is one of the most common formats for storing tabular data. We previously used the `csv` library to handle them as text files, but it was not very convenient. To recall, the syntax for reading a CSV file and displaying the first lines was as follows:

```{python}
import csv

rows = []

with open("data/departement2021.csv") as file_in:
    csv_reader = csv.reader(file_in)
    for row in csv_reader:
        rows.append(row)

rows[:5]
```

With Pandas, just use the `read_csv()` function to import the file as a DataFrame, then the `head()` function.

```{python}
df_departements = pd.read_csv('data/departement2021.csv')
df_departements.head()
```

It is also possible to import a CSV file directly from a URL. This is particularly convenient when the data is regularly updated on a website, and we want to access the latest version without manually downloading the file each time. Let's take the example of a CSV file available on the INSEE website: the file of given names, from civil status data. We also note another handy feature: the CSV file is compressed (in `zip` format), but Pandas can recognize and decompress it before importing.

```{python}
# Importing a CSV file from a URL
url = "https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip"
df_prenoms_url = pd.read_csv(url, sep=";")
df_prenoms_url.head()
```

When working with CSV files, there are many optional arguments available in the `read_csv()` function that allow us to adjust the import process according to the specifics of the file. These arguments can, for example, define a specific delimiter (as above for the given names file), skip certain lines at the beginning of the file, or define data types for each column, and many others. All these parameters and their usage are detailed in the [official documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).

#### Exporting to CSV format

Once the data has been processed and modified within Pandas, it is common to want to export the result as a CSV file for sharing, archiving, or use in other tools. Pandas offers a simple method for this operation: `to_csv()`. Suppose we want to export the data from the `df_departements` DataFrame specific to the five overseas departments.

```{python}
df_departements_dom = df_departements[df_departements["DEP"].isin(["971", "972", "973", "974", "975"])]
df_departements_dom.to_csv('output/departements2021_dom.csv')
```

One of the key arguments of the `to_csv()` method is `index`. By default, `index=True`, which means that the DataFrame's index will also be written in the CSV file. We can verify this by printing the first lines of our CSV file: Pandas has added an unnamed column, which contains the index of the retained rows.

```{python}
with open("output/departements2021_dom.csv") as file_in:
    for i in range(5):
        row = next(file_in).strip()
        print(row)
```

In some cases, notably when the index does not provide useful information or is simply automatically generated by Pandas, we might want to exclude it from the exported file. To do this, we can set `index=False`.

```{python}
df_departements_dom.to_csv('output/departements2021_dom_noindex.csv', index=False)
```

#### Importing a Parquet file

The Parquet format is another format for storing tabular data, increasingly used. Without going into technical details, the Parquet format has various characteristics that make it a preferred choice for storing and processing large volumes of data. Due to these advantages, this format is increasingly used for data dissemination at INSEE. It is therefore essential to know how to import and query Parquet files with Pandas.

Importing a Parquet file into a Pandas DataFrame is as easy as for a CSV file. The function is called `read_parquet()`.

```{python}
df_departements = pd.read_parquet('data/departement2021.parquet')
df_departements.head()
```

#### Exporting to Parquet format

Again, everything works as in the CSV world: we use the `to_parquet()` method to export a DataFrame to a Parquet file. Similarly, we can choose to export or not the index, using the `index` parameter (which defaults to `True`).

```{python}
df_departements_dom = df_departements[df_departements["DEP"].isin(["971", "972", "973", "974", "975"])]
df_departements_dom.to_parquet('output/departements2021_dom.parquet', index=False)
```

One of the major strengths of the Parquet format, compared to text formats like CSV, is its ability to store metadata, i.e., data that helps better understand the data contained in the file. In particular, a Parquet file includes in its metadata the data schema (variable names, variable types, etc.), making it a very suitable format for data dissemination. Let's verify this behavior by revisiting the DataFrame we defined earlier.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, None, None, 0, None],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df = df.assign(
    experiment=pd.Categorical(df["experiment"]),
    date=pd.to_datetime(df["date"])
)
```

This time, we use two specific data types, for categorical data (`category`) and for temporal data (`datetime`). We will see later in the tutorial how to use these types. For now, let's simply note that Pandas stores these types in the data schema.

```{python}
df.info()
```

Let's now verify that exporting and re-importing this data in Parquet preserves the schema.

```{python}
df.to_parquet("output/df_test_schema.parquet", index=False)
df_test_schema_parquet = pd.read_parquet('output/df_test_schema.parquet')

df_test_schema_parquet.info()
```

Conversely, a CSV file, which by definition only contains text, does not preserve this data. The variables for which we specified the type are imported as strings (type `object` in Pandas).

```{python}
df.to_csv("output/df_test_schema.csv", index=False)
df_test_schema_csv = pd.read_csv('output/df_test_schema.csv')

df_test_schema_csv.info()
```

### Viewing a sample of data

When working with large datasets, it is often useful to quickly view a sample of the data to get an idea of its structure, format, or even to detect potential problems. Pandas offers several methods for this.

The `head()` method displays the first rows of the DataFrame. By default, it returns the first 5 rows, but we can specify another number as an argument if necessary.

```{python}
df_departements.head()
```

```{python}
df_departements.head(10)
```

Conversely, the `tail()` method gives a preview of the last rows of the DataFrame.

```{python}
df_departements.tail()
```

Displaying the first or last rows may sometimes not be representative of the entire dataset, especially when the data is sorted. To minimize the risk of obtaining a biased overview of the data, we can use the `sample()` method, which selects a random sample of rows. By default, it returns a single row, but we can request a specific number of rows using the `n` argument.

```{python}
df_departements.sample(n=5)
```

### Getting an overview of the data

One of the first steps when exploring new data is to understand the general structure of the dataset. The `info()` method in Pandas provides a quick overview of the data, including data types, the presence of missing values, and memory usage.

```{python}
df.info()
```

Several key pieces of information can be extracted from this result:

- **index**: The DataFrame has a `RangeIndex`, which means the index is a simple numeric sequence. Here, the index ranges from 0 to 5, for a total of 6 entries.

- **schema**: The list of columns is displayed with very useful information about the data schema:

  - **Non-Null Count**: The number of **non-missing** (non-`nan`) values in the column. If this number is less than the total number of entries (in our case, 6), it means the column contains missing values. Note the possible ambiguity on "null": this indeed means missing values, not values equal to 0. Thus, in our case, the number of "non-null" values for the `var1` variable is 5.

  - **Dtype**: The data type of the column, which helps understand the nature of the information stored in each column. For example, `float64`

 (real numbers), `int32` (integers), `category` (categorical variable), `datetime64[ns]` (temporal information), and `object` (text or mixed data).

Using `info()` is a quick and effective way to get an overview of a DataFrame, quickly identify columns containing missing values, and understand the data structure.

### Calculating descriptive statistics

In addition to the information returned by the `info()` method, we might want to obtain simple descriptive statistics to quickly visualize the distributions of variables. The `describe()` method provides a synthetic view of the distribution of data in each column.

```{python}
df.describe()
```

It should be noted that `describe()` only returns statistics for numeric columns by default. If we want to include columns of other types, we need to specify this via the `include` argument. For example, `df.describe(include='all')` will return statistics for all columns, including metrics such as the unique count, the most frequent value, and the frequency of the most frequent value for non-numeric columns.

```{python}
df.describe(include='all')
```

Note again that the `count` variable returns the number of **non-missing** values in each variable.

## Main data manipulations

### Transforming data

Data transformation operations are essential for shaping, cleaning, and preparing data for analysis. Transformations can apply to the entire DataFrame, specific columns, or specific rows.

#### Transforming a DataFrame

To transform an entire DataFrame (or a sub-DataFrame), it is possible to use vectorized functions, which allow quickly applying an operation to all elements of the DataFrame. This includes a number of methods available for `Series`, as well as NumPy mathematical functions, etc.

For example, raising each numeric value in a DataFrame to the power of 2:

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, None, None, 0, None],
        "var2": np.random.randint(-10, 10, 6),
    }
)

df ** 2
```

or taking the absolute value:

```{python}
np.abs(df)
```

Some methods available for `Series` can also be used to transform an entire DataFrame. For example, the very useful [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) method, which allows replacing all occurrences of a given value with another value. For example, suppose the value 0 in the `var1` column actually indicates a measurement error. It would be preferable to replace it with a missing value.

```{python}
df.replace(0, None)
```

::: {.callout-warning title="Assignment or in-place methods?"}
In the previous example, applying the [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) method does not directly modify the DataFrame. To make the modification persistent, one possibility is to assign the result to an object:

```{python}
df = df.replace(0, None)
```

A second possibility is, when methods offer it, to use the `inplace` argument. When `inplace=True`, the operation is performed "in place", and the DataFrame is therefore directly modified.

```{python}
df.replace(0, None, inplace=True)
```

In practice, it is better to limit `inplace` operations. They do not favor the reproducibility of analyses, as the re-execution of the same cell will give different results each time.
:::

#### Transforming columns

In some cases, we will not want to apply transformations to the entire data but to specific variables. Transformations possible at the DataFrame level (vectorized functions, methods like [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html), etc.) naturally remain possible at the column level.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, None, None, 0, None],
        "var2": np.random.randint(-10, 10, 6),
    }
)

np.abs(df["var2"])
```

```{python}
df["var1"].replace(0, None)
```

But there are other transformations that are generally applied at the level of one or a few columns. For example, when the schema has not been properly recognized upon import, it may happen that numeric variables are defined as strings (type `object` in Pandas).

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, None],
        "var2": ["1", "5", "18"],
    }
)

df.info()
```

In this case, we can use the `astype` method to convert the column to the desired type.

```{python}
df['var2'] = df['var2'].astype(int)

df.info()
```

Another frequent operation is renaming one or more columns. To do this, we can use the [rename()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) method, passing a dictionary containing as many key-value pairs as variables to be renamed, where each key-value pair is of the form `'old_name': 'new_name'`.

```{python}
df.rename(columns={'var2': 'age'})
```

Finally, we might want to remove columns from the DataFrame that are not or no longer useful for analysis. For this, we use the [drop()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method, to which we pass either a string (name of a column if we want to remove only one) or a list of column names to remove.

```{python}
df.drop(columns=['var1'])
```

#### Transforming rows

In statistics, we generally apply transformations involving one or more columns. However, in some cases, it is necessary to apply transformations at the row level. For this, we can use the [apply()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html) method of Pandas, applied to the row axis (`axis=1`). Let's illustrate its operation with a simple case. First, we generate data.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04"],
    }
)

df.head()
```

We now apply the `apply()` function to the DataFrame to calculate a new variable that is the sum of the two existing ones.

```{python}
df['sum_row'] = df.apply(lambda row: row['var1'] + row['var2'], axis=1)

df.head()
```

::: {.callout-tip title="Lambda functions"}
A `lambda` function is a small anonymous function. It can take any number of arguments but can have only one expression. In the example above, the `lambda` function takes a row as an argument and returns the sum of the `var1` and `var2` columns for that row.

Lambda functions allow defining simple functions "on the fly" without having to give them a name. In our example, this would have been perfectly equivalent to the following code:

```{python}
def sum_row(row):
    return row['var1'] + row['var2']

df['sum_row'] = df.apply(sum_row, axis=1)
```

:::

Although `apply()` offers great flexibility, it is not the most efficient method, especially for large datasets. Vectorized operations are always preferable as they process data in blocks rather than row by row. In our case, it would have been preferable to create our variable using column operations.

```{python}
df['sum_row_vect'] = df['var1'] + df['var2']

df.head()
```

However, we might find ourselves in certain (rare) cases where an operation cannot be easily vectorized or where the logic is complex. Suppose, for example, we want to combine the values of several columns based on certain conditions.

```{python}
def combine_columns(row):
    if row['var1'] > 6:
        return str(row['var2'])
    else:
        return str(row['var2']) + "_" + row['date']

df['combined_column'] = df.apply(combine_columns, axis=1)

df
```

### Sorting values

Sorting data is particularly useful for exploring and visualizing data. With Pandas, we use the [sort_values()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) method to sort the values of a DataFrame based on one or more columns.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04"],
    }
)

df
```

To sort the values based on a single column, just pass the column name as a parameter.

```{python}
df.sort_values(by='var1')
```

By default, the sorting is done in ascending

 order. To sort the values in descending order, just set `ascending=False`.

```{python}
df.sort_values(by='var1', ascending=False)
```

If we want to sort the DataFrame on multiple columns, we can provide a list of column names. We can also choose to sort in ascending order for some columns and descending order for others.

### Aggregating data

Aggregating data is a process where the data is broken down into groups based on certain criteria and then aggregated using an aggregation function applied independently to each group. This operation is common in exploratory analysis or when preprocessing data for visualization or statistical modeling.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, None, None, 0, None],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df.head()
```

#### The `groupBy` operation

The `groupBy` method in Pandas allows dividing the DataFrame into subsets based on the values of one or more columns, and then applying an aggregation function to each subset. It returns a `DataFrameGroupBy` object that is not very useful by itself but is an essential intermediate step to then apply one or more aggregation function(s) to the different groups.

```{python}
df.groupby('experiment')
```

#### Aggregation functions

Once the data is grouped, we can apply aggregation functions to obtain a statistical summary. Pandas includes a number of these functions, the complete list of which is detailed in the [documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#built-in-aggregation-methods). Here are some examples of using these methods.

For example, count the number of occurrences in each group.

```{python}
df.groupby('experiment').size()
```

Calculate the sum of a variable by group.

```{python}
df.groupby('experiment')['var1'].sum()
```

Or count the number of unique values of a variable by group. The possibilities are numerous.

```{python}
# For the number of unique values of 'var2' in each group
df.groupby('experiment')['var2'].nunique()
```

When we want to apply multiple aggregation functions at once or custom functions, we use the `agg` method. This method accepts a list of functions or a dictionary that associates column names with functions to apply. This allows for finer application of aggregation functions.

```{python}
df.groupby('experiment').agg({'var1': 'mean', 'var2': 'count'})
```

::: {.callout-note title="Method chaining"}
The previous examples illustrate an important concept in Pandas: method chaining. This term refers to the possibility of chaining transformations applied to a DataFrame by applying methods to it in a chain. At each applied method, an intermediate DataFrame is created (but not assigned to a variable), which becomes the input of the next method.

Method chaining allows combining several operations into a single code expression. This can improve efficiency by avoiding intermediate assignments and making the code more fluid and readable. It also favors a functional programming style where data flows smoothly through a chain of transformations.
:::

#### Effects on the index

It is interesting to note the effects of the aggregation process on the DataFrame's index. The last example above illustrates this well: the groups, i.e., the modalities of the variable used for aggregation, become the values of the index.

We might want to reuse this information in subsequent analyses and therefore want it as a column. For this, just reset the index with the `reset_index()` method.

```{python}
df_agg = df.groupby('experiment').agg({'var1': 'mean', 'var2': 'count'})
df_agg.reset_index()
```

### Handling missing values

Missing values are a common reality in real-world data processing and can occur for various reasons, such as non-responses to a questionnaire, data entry errors, data loss during transmission, or simply because the information is not applicable. Pandas offers several tools to handle missing values.

#### Representation of missing values

In Pandas, missing values are generally represented by `np.nan`, a special marker provided by the `NumPy` library. While it is preferable to use this object to denote missing values, note that the `None` object in `Python` is also understood as a missing value by `Pandas`.

Let's verify this property. To identify where the missing values are, we use the `isna()` function, which returns a boolean DataFrame indicating `True` where the values are `NaN`.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", None, "train", "validation"],
        "sample": "sample1"
    }
)

df.isna()
```

#### Calculations on columns containing missing values

During statistical calculations, missing values are generally ignored. For example, the `.mean()` method calculates the mean of non-missing values.

```{python}
df['var1'].mean()
```

However, calculations involving multiple columns do not always ignore missing values and can often result in `NaN`.

```{python}
df['var3'] = df['var1'] + df['var2']

df
```

#### Removing missing values

The `dropna()` method allows us to remove rows (`axis=0`) or columns (`axis=1`) containing missing values. By default, any row containing at least one missing value is removed.

```{python}
df.dropna()
```

By changing the `axis` parameter, we can request that any column containing at least one missing value be removed.

```{python}
df.dropna(axis=1)
```

Finally, the `how` parameter defines the deletion mode. By default, a row or column is removed when at least one value is missing (`how=any`), but it is possible to remove the row/column only when all values are missing (`how=all`).

#### Replacing missing values

To handle missing values in a DataFrame, a common approach is imputation, which involves replacing the missing values with other values. The `fillna()` method allows us to perform this operation in various ways. One possibility is replacement by a constant value.

```{python}
df['var1'].fillna(value=0)
```

::: {.callout-warning title="Changing the representation of missing values"}
It can sometimes be tempting to change the manifestation of a missing value for visibility reasons, for example by replacing it with a string:

```{python}
df['var1'].fillna(value="MISSING")
```

In practice, this is not recommended. It is indeed preferable to stick to Pandas' standard convention (using `np.nan`), firstly for standardization purposes that facilitate reading and maintaining the code, but also because the standard convention is optimized for performance and calculations from data containing missing values.
:::

Another frequent imputation method is to use a statistical value, such as the mean or median of the variable.

```{python}
df['var1'].fillna(value=df['var1'].mean())
```

::: {.callout-warning title="Imputation bias"}
Replacing missing values with a constant value, such as zero, the mean, or the median, can be problematic. If the data is not missing at random (MNAR), this can introduce bias into the analysis. MNAR variables are variables whose probability of being missing is related to their own value or other variables in the data. In such cases, more sophisticated imputation may be necessary to minimize distortions. We will see an example in the end-of-tutorial exercise.
:::

### Handling data of specific types

#### Text data

Text data often requires cleaning and preparation before analysis. Pandas provides an array of vectorized operations via the `str` library that make preparing text data both simple and very efficient. Again, the possibilities are numerous and detailed in the [documentation](https://pandas.pydata.org/docs/user_guide/text.html). Here we present the most frequently used methods in data analysis.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "test", "train", "validation"],
        "sample": ["  sample1", "sample1", "sample2", "   sample2   ", "sample2  ", "sample1"]
    }
)

df
```

A first frequent operation is to extract certain characters from a string. We use the `str[n:]` function (with a somewhat peculiar syntax) for this. For example, if we want to extract the last character of the `sample` variable to retain only the sample number.

```{python}
df["sample_n"] = df["sample"].str[-1:]

df
```

The principle was correct, but the presence of extraneous spaces in our text data (which were not visible when viewing the DataFrame!) made the operation more difficult than expected. This is an opportunity to introduce the `strip` family of methods (`.str.strip()`, `.str.lstrip()`, and `.str.rstrip()`) that respectively remove extr

aneous spaces from both sides or one side.

```{python}
df["sample"] = df["sample"].str.strip()
df["sample_n"] = df["sample"].str[-1:]

df
```

We might also want to filter a DataFrame based on the presence or absence of a certain string (or substring) of characters. For this, we use the `.str.contains()` method.

```{python}
df[df['experiment'].str.contains('test')]
```

Finally, we might want to replace a string (or substring) of characters with another, which the `str.replace()` method allows.

```{python}
df['experiment'] = df['experiment'].str.replace('validation', 'val')

df
```

#### Categorical data

Categorical data is variables that contain a limited number of categories. Similar to `R` with the notion of `factor`, Pandas has a special data type, `category`, which is useful for representing categorical data more efficiently and informatively. Categorical data is indeed optimized for certain types of data and can speed up operations like grouping and sorting. It is also useful for visualization, ensuring that categories are displayed in a coherent and logical order.

To convert a variable to the `category` format, we use the `astype()` method.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", None, "train", "validation"],
    }
)
print(df.dtypes)
```

```{python}
df['experiment'] = df['experiment'].astype('category')

print(df.dtypes)
```

This conversion gives us access to some very useful methods, specific to handling categorical variables. For example, it can be useful to rename categories for clarity or standardization.

```{python}
df['experiment'] = df['experiment'].cat.rename_categories({'test': 'Test', 'train': 'Train', 'validation': 'Validation'})
df
```

Sometimes, the order of categories is significant, and we might want to modify it. This is particularly important for visualization, as the categories will by default be displayed in the specified order.

```{python}
df_cat = df['experiment'].cat.reorder_categories(['Test', 'Train', 'Validation'], ordered=True)
df.groupby("experiment").mean().plot(kind='bar')
```

#### Temporal data

Temporal data is often present in tabular data to temporally identify the observations collected. Pandas offers functionalities for handling these types of data, particularly through the `datetime64` type, which allows precise manipulation of dates and times.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2023-01-01", "2023-01-02"],
        "sample": ["sample1", "sample1", "sample2", "sample2"]
    }
)

df.dtypes
```

To handle temporal data, it is necessary to convert the strings into `datetime` objects. Pandas does this via the `to_datetime()` function.

```{python}
df['date'] = pd.to_datetime(df['date'])

df.dtypes
```

Once converted, dates can be formatted, compared, and used in calculations. In particular, Pandas now understands the "order" of the dates present in the data, allowing for filtering over given periods.

```{python}
df[(df['date'] >= "2022-01-01") & (df['date'] < "2022-01-03")]
```

We might also want to perform less precise filtering, involving the year or month. Pandas allows us to easily extract specific components of the date, such as the year, month, day, hour, etc.

```{python}
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day

df[df['year'] == 2023]
```

Finally, calculations involving dates become possible. We can add or subtract time periods from dates and compare them with each other. The functions used come from `Pandas` but are very similar in operation to those of the [time](https://docs.python.org/fr/3/library/time.html) module in Python.

For example, we can add time intervals or calculate differences from a reference date.

```{python}
df['date_plus_one'] = df['date'] + pd.Timedelta(days=1)
df['date_diff'] = df['date'] - pd.to_datetime('2022-01-01')

df
```

### Joining tables

In data analysis, it is common to want to combine different data sources. This combination can be done vertically (one DataFrame on top of another), for example, when combining two years of the same survey for joint analysis. The combination can also be done horizontally (side by side) based on one or more join keys, often to enrich one data source with information from another source covering the same statistical units.

#### Concatenating tables

The vertical concatenation of tables is done using the `concat()` function in Pandas.

```{python}
df1 = pd.DataFrame(
    data = {
        "var1": [1, 5],
        "var2": [3, 7],
        "date": ["2022-01-01", "2022-01-02"],
        "sample": ["sample1", "sample1"]
    }
)

df2 = pd.DataFrame(
    data = {
        "var1": [9, 13],
        "date": ["2023-01-01", "2023-01-02"],
        "var2": [11, 15],
        "sample": ["sample2", "sample2"]
    }
)

df_concat = pd.concat([df1, df2])

df_concat
```

Note that the order of variables in the two DataFrames is not important. Pandas does not "dumbly" juxtapose the two DataFrames; it matches the schemas to align the variables by name. If two variables have the same name but not the same type - for example, if a numeric variable has been interpreted as strings - Pandas will resolve the issue by taking the common denominator, usually converting to strings (type `object`).

However, the previous concatenation reveals an issue of repetition at the index level. This is logical: we did not specify an index for our initial two DataFrames, which therefore have the same position index ([0, 1]). In this case (where the index is not important), we can pass the `ignore_index=True` parameter to rebuild the final index from scratch.

```{python}
df_concat = pd.concat([df1, df2], ignore_index=True)

df_concat
```

::: {.callout-warning title="Iterative construction of a DataFrame"}
One might have the idea of using `pd.concat()` to iteratively construct a DataFrame by adding a new row to the existing DataFrame in each iteration of a loop. However, this is not a good idea: as we have seen, a DataFrame is represented in memory as a juxtaposition of Series. Thus, adding a column to a DataFrame is not costly, but adding a row involves modifying each element constituting the DataFrame. To construct a DataFrame, it is therefore advisable to store the rows in a list of lists (one per column) or a dictionary, then call `pd.DataFrame()` to build the DataFrame, as we did at the beginning of this tutorial.
:::

#### Merging tables

Merging tables is an operation that allows us to associate rows from two different DataFrames based on one or more common keys, similar to joins in SQL databases. Different types of joins are possible depending on the data we want to keep, the main ones being represented in the following diagram.

![](img/joins.png)

Source: [link](https://medium.com/swlh/merging-dataframes-with-pandas-pd-merge-7764c7e2d46d)

In Pandas, joins are done with the [merge()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) function. To perform a join, we must specify (at a minimum) two pieces of information:

- the type of join: by default, Pandas performs an `inner` join. The `how` parameter allows specifying other types of joins;

- the join key. By default, Pandas tries to join the two DataFrames based on their indexes. In practice, we often specify a variable present in the DataFrames as the join key (the `on` parameter if the variable has the same name in both DataFrames, or `left_on` and `right_on` otherwise).

Let's analyze the difference between the different types of joins through examples.

```{python}
df_a = pd.DataFrame({
    'key': ['K0', 'K1', 'K2', 'K3', 'K4'],
    'A': ['A0', 'A1', 'A2', 'A3', 'A4'],
    'B': ['B0', 'B1', 'B2', 'B3', 'A4']
})

df_b = pd.DataFrame({
    'key': ['K0', 'K1', 'K2', 'K5', 'K6'],
    'C': ['C0', 'C1', 'C2', 'C5', 'C6'],
    'D': ['D0', 'D1', 'D2', 'D5', 'D6']
})

display(df_a)
display(df_b)
```

The `inner` join keeps the observations whose key is present in both DataFrames.

```{python}
df_merged_inner = pd.merge(df_a, df_b, on='key')
df_merged_inner
```

::: {.callout-warning title="Inner joins"}
The `inner` join is the most intuitive: it generally does not create missing values and therefore allows working directly on the merged table. But beware: if many keys are not present in both DataFrames, an `inner` join can result in significant data loss, leading to biased final results. In this case, it is better to choose a left or right join, depending on the source we want to enrich and for which it is most important to minimize data loss.
:::

A `left` join keeps all observations in the left DataFrame (the first DataFrame specified in `pd.merge()`). As a result, if keys are present in the left DataFrame but not in the right one, the final DataFrame contains missing values at those observations (for the right DataFrame's variables).

```{python}
df_merged_left = pd.merge(df_a, df_b, how="left", on='key')
df_merged_left
```

The `outer` join contains all observations and variables in both DataFrames. Thus, the retained information is maximal, but on the other hand, missing values can be quite numerous. It will therefore be necessary to handle missing values well before proceeding with analyses.

```{python}
df_merged_outer = pd.merge(df_a, df_b, how="outer", on='key')
df_merged_outer
```

## Exercises

### Comprehension questions

1. What is a DataFrame in the context of Pandas, and what type of data structure can it be compared to in Python?
2. What is the fundamental difference between a NumPy array and a Pandas Series?
3. What is the relationship between Series and DataFrame in Pandas?
4. How are data structured in a Pandas DataFrame?
5. What is the role of the index in a Pandas DataFrame, and how can it be used when manipulating data?
6. What methods can you use to explore an unknown DataFrame and learn more about its content and structure?
7. In Pandas, what is the difference between assigning the result of an operation to a new variable and using a method with the `inplace=True` argument?
8. How does the principle of vectorization apply in Pandas, and why is it advantageous for manipulating data?
9. How does Pandas represent missing values, and what impact does this have on calculations and data transformations?
10. What is the difference between concatenating two DataFrames and joining them via a merge, and when would you use one over the other?

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

1. A DataFrame in Pandas is a two-dimensional data structure, comparable to a table or an Excel spreadsheet. In the Python context, it can be compared to a dictionary of NumPy arrays, where the keys are column names, and the values are the columns themselves.

2. The main difference between a NumPy array and a Pandas Series is that the Series can contain labeled data, meaning it has an associated index that allows access and manipulation by label.

3. A DataFrame is essentially a collection of Series. Each column of a DataFrame is a Series, and all these Series share the same index, which corresponds to the row labels of the DataFrame.

4. Data in a Pandas DataFrame are structured in columns and rows. Each column can contain a different type of data (numeric, string, boolean, etc.), and each row represents an observation.

5. The index in a Pandas DataFrame serves to uniquely identify each row in the DataFrame. It allows quick access to rows, performing joins, sorting data, and facilitating grouping operations.

6. To explore an unknown DataFrame, you can use `df.head()` to see the first rows, `df.tail()` for the last rows, `df.info()` to get a summary of data types and missing values, and `df.describe()` for descriptive statistics.

7. Assigning the result of an operation to a new variable creates a copy of the DataFrame with the applied modifications. Using a method with `inplace=True` modifies the original DataFrame without creating a copy, which can be more memory-efficient.

8. Pandas represents missing values with the `nan` (Not a Number) object from `NumPy` for numeric data and with `None` or `pd.NaT` for date/time data. These missing values are generally ignored in statistical calculations, which can affect the results if they are not handled properly.

9. Concatenating consists of stacking DataFrames vertically or aligning them horizontally, primarily used when the DataFrames have the same schema or when you want to stack the data. Merging, inspired by SQL JOIN operations, combines DataFrames based on common key values and is used to enrich one dataset with information from another.

</details>

:::

### Multiple ways to create a DataFrame

In the following cell, we have retrieved cash register data on sales from different stores. The data is presented in two different ways: one as observations (each list contains data from a row), and the other as variables (each list contains data from a column).

```{python}
data_list1 = [
    ['Carrefour', '01.1.1', 3, 1.50],
    ['Casino', '02.1.1', 2, 2.30],
    ['Lidl', '01.1.1', 7, 0.99],
    ['Carrefour', '03.1.1', 5, 5.00],
    ['Casino', '01.1.1', 10, 1.20],
    ['Lidl', '02.1.1', 1, 3.10]
]

data_list2 = [
    ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    [3, 2, 7, 5, 10, 1],
    [1.50, 2.30, 0.99, 5.00, 1.20, 3.10]
]
```

The goal is to build in both cases the same DataFrame containing each of the 6 observations and 4 variables, with the same names in both DataFrames. Each case will correspond to a more suitable input data structure, dictionary, or list of lists... make the right choice! We will verify that the two DataFrames are identical using the [equals()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.equals.html) method.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
data_list1 = [
    ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    [3, 2, 7, 5, 10, 1],
    [1.50, 2.30, 0.99, 5.00, 1.20, 3.10]
]

data_list2 = [
    ['Carrefour', '01.1.1', 3, 1.50],
    ['Casino', '02.1.1', 2, 2.30],
    ['Lidl', '01.1.1', 7, 0.99],
    ['Carrefour', '03.1.1', 5, 5.00],
    ['Casino', '01.1.1', 10, 1.20],
    ['Lidl', '02.1.1', 1, 3.10]
]

# If the data is in column form: from a dictionary
data_dict = {
    'store': data_list1[0],
    'product': data_list1[1],
    'quantity': data_list1[2],
    'price': data_list1[3]
}

df_from_dict = pd.DataFrame(data_dict)

# If the data is in row form: from a list of lists
columns = ['store', 'product', 'quantity', 'price']
df_from_list = pd.DataFrame(data_list2, columns=columns)

# Verification
df_from_dict.equals(df_from_list)
```

</details>

:::

### Data selection in a DataFrame

A Pandas DataFrame is created with cash register data (same data as the previous exercise).

```{python}
data = {
    'store': ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    'product': ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    'quantity': [3, 2, 7, 5, 10, 1],
    'price': [1.50, 2.30, 0.99, 5.00, 1.20, 3.10],
    'date_time': pd.to_datetime(["2022-01-01 14:05", "2022-01-02 09:30",
                                 "2022-01-03 17:45", "2022-01-04 08:20",
                                 "2022-01-05 19:00", "2022-01-06 16:30"])
}

df = pd.DataFrame(data)
```

Use the `loc` and `iloc` methods to select specific data:

- Select the data from the first row.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.iloc[0])
```

</details>

:::

- Select all data from the "price" column.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[:, 'price'])
```

</details>

:::

- Select the rows corresponding to the store "Carrefour" only.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['store'] == 'Carrefour'])
```

</details>

:::

- Select the quantities purchased for products classified "01.1.1" (Bread).

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['product'] == '01.1.1', 'quantity'])
```

</details>

:::

- Select the data from the "store" and "price" columns for all rows.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[:, ['store', 'price']])
```

</details>

:::

- Select the rows where the purchased quantity is greater than 5.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['quantity'] > 5])
```

</details>

:::

- Filter to select all transactions that occurred after 3 PM.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['date_time'].dt.hour > 15])
```

</details>

:::

- Select the transactions that took place on "2022-01-03".

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['date_time'].dt.date == pd.to_datetime('2022-01-03').date()])
```

</details>

:::

### Exploring the first names file

The first names file contains data on the first names given to children born in France between 1900 and 2021. This data is available at the national, department, and regional levels at the following address: [https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262](https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262). The goal of this tutorial is to propose an analysis of this file, from data cleaning to first name statistics.

#### Part 1: Import and data exploration

- Import the data into a DataFrame using this [URL](https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip).
- View a sample of the data. Do you notice any anomalies?
- Display the main information about the DataFrame. Identify any variables with incorrect types or any missing values.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
url = "https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip"
df_first_names = pd.read_csv(url, sep=";")

df_first_names.head(10)
df_first_names.sample(n=50)

df_first_names.info()
```

</details>

:::

#### Part 2: Data cleaning

- The output of the `info()` method suggests missing values in the first names column. Display these rows. Verify that these missing values are correctly specified.
- The output of the `head()` method shows a recurring "_PRENOMS_RARES" modality in the first names column. What proportion of the individuals in the database does this represent? Convert these values to `np.nan`.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df_first_names[df_first_names["preusuel"].isna()])
prop_rares = df_first_names.groupby("preusuel")["nombre"].sum()["_PRENOMS_RARES"] / df_first_names["nombre"].sum()
print(prop_rares)  # ~ 2% of the database
df_first_names = df_first_names.replace('_PRENOMS_RARES', np.nan)
```

</details>

:::

- We notice that the first names of people whose year of birth is unknown are grouped under the "XXXX" modality. What proportion of the individuals in the database does this represent? Convert these values to `np.nan`.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
prop_xxxx = df_first_names.groupby("annais")["nombre"].sum()["XXXX"] / df_first_names["nombre"].sum()
print(prop_xxxx)  # ~ 1% of the database
df_first_names = df_first_names.replace('XXXX', np.nan)
```

</details>

:::

- Remove the rows containing missing values from the sample.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_first_names = df_first_names.dropna()
```

</details>

:::

- Convert the `annais` column to numeric type and the `sexe` column to categorical type.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_first_names['annais'] = pd.to_numeric(df_first_names['annais'])
df_first_names['sexe'] = df_first_names['sexe'].astype('category')
```

</details>

:::

- Verify with the `info()` method that the cleaning has been correctly applied.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_first_names.info()
```

</details>

:::

#### Part 3: Descriptive statistics on births

- The [documentation](https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262#documentation) of the file informs us that the data can be considered quasi-exhaustive from 1946 onwards. For this part only, filter the data to keep only data from 1946 onwards.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_first_names_post_1946 = df_first_names[df_first_names["annais"] >= 1946]
```

</details>

:::

- Calculate the total number of births by sex.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
births_per_sex = df_first_names_post_1946.groupby('sexe')['nombre'].sum()
print(births_per_sex)
```

</details>

:::

- Identify the five years with the highest number of births.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
top5_years = df_first_names_post_1946.groupby('annais')['nombre'].sum().nlargest(5)
print(top5_years)
```

</details>

:::

#### Part 4: First name analysis

- Identify the total number of unique first names in the DataFrame.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
total_unique_names = df_first_names['preusuel'].nunique()
print(total_unique_names)
```

</details>

:::

- Count the number of people with a single-letter first name.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
single_letter_names = df_first_names[df_first_names['preusuel'].str.len() == 1]['nombre'].sum()
print(single_letter_names)
```

</details>

:::

- Create a "popularity function" that, for a given first name, displays the year it was most given and the number of times it was given that year.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
def popularity_by_year(df, first_name):
    # Filter the DataFrame to keep only the rows corresponding to the given first name
    df_first_name = df[df['preusuel'] == first_name]

    # Group by year, sum the births, and identify the year with the maximum births
    df_agg = df_first_name.groupby('annais')['nombre'].sum()
    max_year = df_agg.idxmax()
    max_n = df_agg[max_year]

    print(f"The first name '{first_name}' was most given in {max_year}, with {max_n} births.")

# Test the function with an example
popularity_by_year(df_first_names, 'ALFRED')
```

</details>

:::

- Create a function that, for a given sex, returns a DataFrame containing the most given first name for each decade.

```{python}
# Test

 your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
def popularity_by_decade(df, sex):
    # Filter by sex
    df_sub = df[df["sexe"] == sex]

    # Calculate the decade variable
    df_sub["decade"] = (df_sub["annais"] // 10) * 10

    # Calculate the sum of births for each first name and each decade
    df_counts_decade = df_sub.groupby(["preusuel", "decade"])["nombre"].sum().reset_index()

    # Find the index of the most frequent first name for each decade
    idx = df_counts_decade.groupby("decade")["nombre"].idxmax()

    # Use the index to obtain the corresponding rows from the df_counts_decade DataFrame
    df_popularity_decade = df_counts_decade.loc[idx].set_index("decade")

    return df_popularity_decade

# Test the function with an example
popularity_by_decade(df_first_names, sex=2)
```

</details>

:::

### Calculation of a carbon footprint per inhabitant at the municipal level

The goal of this exercise is to calculate a carbon footprint per inhabitant at the municipal level. To do this, we will need to combine two data sources:

- Legal populations at the municipal level from the population census ([source](https://www.insee.fr/fr/statistiques/6683037))

- Greenhouse gas emissions estimated at the municipal level by ADEME ([source](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_))

This exercise constitutes a simplified version of a [complete practical exercise on Pandas](https://pythonds.linogaliana.fr/content/manipulation/02b_pandas_TP.html#importer-les-donn%C3%A9es) proposed by Lino Galiana in his [course at ENSAE](https://pythonds.linogaliana.fr/).

#### Part 1: Exploring the legal municipal populations data

- Import the CSV file `communes.csv`.
- Use the `.sample()`, `.info()`, and `.describe()` methods to get an overview of the data.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_pop_communes = pd.read_csv("data/communes.csv", sep=";")

df_pop_communes.sample(10)
df_pop_communes.info()
df_pop_communes.describe()
```

</details>

:::

- Identify and remove rows corresponding to municipalities without population.
- Remove the "PMUN" and "PCAP" columns, which are irrelevant for the analysis.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
n_communes_0_pop = df_pop_communes[df_pop_communes["PTOT"] == 0].shape[0]
print(n_communes_0_pop)
df_pop_communes = df_pop_communes[df_pop_communes["PTOT"] > 0]

df_pop_communes = df_pop_communes.drop(columns=["PMUN", "PCAP"])
```

</details>

:::

Do the municipalities with the longest names also have the smallest populations? To find out:
- Create a new variable that contains the number of characters of each municipality using the [str.len()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.len.html) method.
- Calculate the correlation between this variable and the total population using the [corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) method.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_pop_communes_stats = df_pop_communes.copy()
df_pop_communes_stats['length'] = df_pop_communes_stats['COM'].str.len()
df_pop_communes_stats['length'].corr(df_pop_communes_stats['PTOT'])
```

</details>

:::

#### Part 2: Exploring the municipal emissions data

- Import the emissions data from this [URL](https://data.ademe.fr/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/data-files/IGT%20-%20Pouvoir%20de%20r%C3%A9chauffement%20global.csv).
- Use the `.sample()`, `.info()`, and `.describe()` methods to get an overview of the data.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
url_ademe = "https://data.ademe.fr/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/data-files/IGT%20-%20Pouvoir%20de%20r%C3%A9chauffement%20global.csv"
df_emissions = pd.read_csv(url_ademe)

df_emissions.sample(10)
df_emissions.info()
df_emissions.describe()
```

</details>

:::

- Are there rows with missing values for all emission columns? Check using the [isnull()](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) and [all()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.all.html) methods.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions_num = df_emissions.select_dtypes(['number'])
only_nan = df_emissions_num[df_emissions_num.isnull().all(axis=1)]
only_nan.shape[0]
```

</details>

:::

- Create a new column that gives the total emissions per municipality.
- Display the 10 most emitting municipalities. What do you observe in the results?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions['total_emissions'] = df_emissions.sum(axis=1, numeric_only=True)

df_emissions.sort_values(by="total_emissions", ascending=False).head(10)
```

</details>

:::

- It seems that the major emission sectors are "Industry excluding energy" and "Other international transport." To verify if this conjecture holds, calculate the correlation between total emissions and the sectoral emission items using the [corrwith()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corrwith.html) method.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions.corrwith(df_emissions["total_emissions"], numeric_only=True)
```

</details>

:::

- Extract the department number from the municipality code into a new variable.
- Calculate the total emissions by department.
- Display the top 10 emitting departments. Are the results logical compared to the analysis at the municipal level?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions["dep"] = df_emissions["INSEE commune"].str[:2]
df_emissions.groupby("dep").agg({"total_emissions": "sum"}).sort_values(by="total_emissions", ascending=False).head(10)
```

</details>

:::

#### Part 3: Preliminary checks for merging data sources

To perform a merge, it is always preferable to have a join key, i.e., a column common to both sources that uniquely identifies the statistical units. The purpose of this part is to find the relevant join key.

- Check if the variable containing the municipality names contains duplicates.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
duplicates = df_pop_communes.groupby('COM').count()['DEPCOM']
duplicates = duplicates[duplicates > 1]
duplicates = duplicates.reset_index()
duplicates
```

</details>

:::

- Filter in the initial DataFrame the municipalities with duplicated names and sort it by municipality code. Do the duplicates seem problematic?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_pop_communes_duplicates = df_pop_communes[df_pop_communes["COM"].isin(duplicates["COM"])]
df_pop_communes_duplicates.sort_values('COM')
```

</details>

:::

- Verify that the municipality codes uniquely identify the associated municipality.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
(df_pop_communes_duplicates.groupby("DEPCOM")["COM"].nunique() != 1).sum()
```

</details>

:::

- Display the municipalities present in the population data but not in the emissions data, and vice versa. What do you conclude?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
# Observations in the population data but not in the emissions data
df_pop_communes[~df_pop_communes["DEPCOM"].isin(df_emissions["INSEE commune"])]

# Observations in the emissions data but not in the population data
df_emissions[~df_emissions["INSEE commune"].isin(df_pop_communes["DEPCOM"])]
```

</details>

:::

#### Part 4: Calculating a carbon footprint per inhabitant for each municipality

- Merge the two DataFrames using the municipality code as the join key. Note: the variables are not named the same on both sides!

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions_pop = pd.merge(df_pop_communes, df_emissions, how="inner", left_on="DEPCOM", right_on="INSEE commune")
df_emissions_pop
```

</details>

:::

- Calculate a carbon footprint for each municipality, corresponding to the total emissions of the municipality divided by its total population.
- Display the top 10 municipalities with the highest carbon footprints.
- Are the results the same as those with total emissions? What do you conclude?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions_pop["carbon_footprint"] = df_emissions_pop["total_emissions"] / df_emissions_pop["PTOT"]
df_emissions_pop.sort_values("carbon_footprint", ascending=False).head(10)
```

</details>

:::

### Analysis of the evolution of a production index

You have two CSV data sets available in the `data/` folder:

- `serie_glaces_valeurs.csv` contains the monthly values of the production price index of the French ice cream and sorbet industry.
- `serie_glaces_metadonnees.csv` contains the associated metadata, including the codes indicating the data status.

The goal is to use `Pandas` to calculate:

- the evolution of the index between each period (month)
- the evolution of the index on a year-over-year basis (between a given month and the same month the following year).

#### Part 1: Importing data

- Import the two CSV files into DataFrames. Note: in both cases, there are extraneous rows before the data that need to be skipped using the `skiprows` parameter of the [read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function.
- Give simple and relevant names to the various variables.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_values = pd.read_csv('data/serie_glaces_valeurs.csv', delimiter=';',
                        skiprows=4, names=["period", "index", "code"])
df_metadata = pd.read_csv('data/serie_glaces_metadonnees.csv', delimiter=';',
                          skiprows=5, names=["code", "meaning"])
```

</details>

:::

#### Part 2: Filtering relevant data

- Merge the two DataFrames to retrieve the meanings of the codes present in the data.
- Filter the data to keep only the "Normal Value" data.
- Remove the columns related to the codes, which we no longer need for the rest.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_merged = pd.merge(df_values, df_metadata, how='left', on='code')

df_clean = df_merged[df_merged['code'] == "A"]
df_clean = df_clean[["period", "index"]]
```

</details>

:::

#### Part 3: Data preprocessing

Verify if the types of variables are relevant according to their nature. If not, convert them with the appropriate functions.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_clean.info()
df_clean['period'] = pd.to_datetime(df_clean['period'])
df_clean['index'] = pd.to_numeric(df_clean['index'])
df_clean.info()
```

</details>

:::

#### Part 4: Calculating periodic evolution

- Use the [shift()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html) method to create a new column containing the previous month's index.
- Calculate the difference between the current index and the shifted index to obtain the (percentage) evolution from one month to the next.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_clean['previous_index'] = df_clean['index'].shift(1)
df_clean['evolution'] = ((df_clean['index'] - df_clean['previous_index']) / df_clean['previous_index']) * 100

# Alternative method
df_clean['alternative_evolution'] = df_clean['index'].pct_change(periods=1) * 100
```

</details>

:::

#### Part 5: Calculating year-over-year evolution

As you saw in the previous exercise's solution, the [pct_change()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html) method allows you to calculate an evolution between two periods. Use this method to calculate a year-over-year evolution for each month.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_clean["year_over_year_evolution"] = df_clean['index'].pct_change(periods=12) * 100
df_clean.head(20)
```

</details>

:::


:::
